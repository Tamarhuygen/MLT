\documentclass[10pt, a4paper, twoside]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
%\usepackage[utf8x]{inputenc}
\usepackage{amsmath, amssymb, amsthm, amsfonts, mathrsfs, amsfonts}
\usepackage{mathtools} 

\usepackage{enumerate}

\usepackage[noabbrev]{cleveref}

%\usepackage{subfig}
\usepackage{pgf,tikz}
%\usetikzlibrary{arrows}

%\usepackage{natbib}
\usepackage[osf]{mathpazo} % Nice text font
\usepackage{euler} % Very nice mathmode font


% bold
\newcommand{\F}{\ensuremath{\mathbb{F}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
% 
% calligraphic
\newcommand{\A}{\ensuremath{\mathcal{A}}}
\newcommand{\E}{\ensuremath{\mathcal{E}}}
\newcommand{\I}{\ensuremath{\mathcal{I}}}

% Delimiters (requires mathtools package)
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\brac[]
\DeclarePairedDelimiter\cbrac\{\}
\DeclarePairedDelimiter\paren()
\DeclarePairedDelimiter{\ip}\langle\rangle
\DeclarePairedDelimiter{\nrm}\lVert\rVert
\DeclarePairedDelimiter{\ceil}\lceil\rceil

% Power set
\newcommand{\Ps}{\ensuremath{\mathcal{P}}}

 
% other things ...
\renewcommand{\c}{\ensuremath{\colon}}
\newcommand{\se}{\ensuremath{\subseteq}}
\renewcommand{\d}{\ensuremath{\ d}}
\newcommand{\Ind}{\ensuremath{\mathbb{1}}}
\newcommand{\im}{\ensuremath{\mathbf{i}}}

\newcommand{\argmin}{\operatorname*{argmin}}
\newcommand{\argmax}{\operatorname*{argmax}}
% \renewcommand\qedsymbol{\rule{1ex}{1ex}}

% course specifics
\renewcommand{\P}{\operatorname*{\ensuremath{\mathbf{P}}}} % probability measure
\newcommand{\Ev}{\operatorname*{\ensuremath{\mathbf{E}}}} %expected value
\newcommand{\Fa}{\ensuremath{\mathcal{F}}} %sigma-algebra

\newcommand{\sgn}{\mathrm{sgn}}

% Solutions use a modified proof environment
\newenvironment{solution}
               {\let\oldqedsymbol=\qedsymbol
                \renewcommand{\qedsymbol}{$\blacktriangleleft$}
                \begin{proof}[Solution]}
               {\end{proof}
                \renewcommand{\qedsymbol}{\oldqedsymbol}}


\newcommand{\TODO}{\textcolor{red}{\textbf{!!!!!! }}}

\newcommand{\firstName}  {Twan}
\newcommand{\lastName}   {Koperberg}
\newcommand{\studId}     {0713309 (Leiden)}
\renewcommand{\email}    {twankop@gmail.com}

\newcommand{\firstNameII}  {Tamar}
\newcommand{\lastNameII}   {Huygen}
\newcommand{\studIdII}     {10907483 (UvA)}
\newcommand{\studIdIII}    {2556474 (VU)}
\newcommand{\emailII}     {tamar@huygen.nl}

\begin{document}
\begin{center}

  {\huge\bf Machine learning theory}\\
  {\large\sc Homeworkset 2 }\\ \vspace{1em}
  \firstName \textsc{ \lastName}, {\sc s}\studId \\
  \email\text{}\\ \smallskip
  \firstNameII \textsc{ \lastNameII}, \studIdII, \studIdIII\\
  \emailII \\ \bigskip
  \today \\\bigskip
  \hrule
  \bigskip
\end{center}


\section*{Exercise 3.1} 
Let $\mathcal{H}$ be a hypothesis class for a binary classification task. Suppose that $\mathcal{H}$ is PAC learnable and its sample complexity given by $m_{\mathcal{H}}(\cdot,\cdot)$.
Show that $m_{\mathcal{H}}$ is monotonically nonincreasing in each of its parameters. That is, show that given $\delta \in (0,1)$, and given $0<\varepsilon_1 \leq \varepsilon_2<1$, we have that $m_{\mathcal{H}}(\varepsilon_1, \delta) \geq m_{\mathcal{H}}(\varepsilon_2, \delta)$. Similarly, show that given $\varepsilon \in (0,1)$, and given $0<\delta_1 \leq \delta_2 <1$, we have that $m_{\mathcal{H}}(\varepsilon, \delta_1) \geq m_{\mathcal{H}}(\varepsilon, \delta_2)$.
\begin{solution}
  When $\mathcal{H}$ is PAC learnable, then it holds that for a given $\delta$ it holds that there exists a function $m_{\mathcal{H}}(\varepsilon_1,\delta ):(0,1)^2 \rightarrow \N$, and a learning algorithm, with the property that when the learning algorithm is running on $m \geq m_{\mathcal{H}}$ samples, the algorithm returns a hypothesis $h$ such that, with probability of at least $1- \delta$,
  \begin{equation*}
    L_{\mathcal{D}}(h) - \min_{h' \in \mathcal{H}} L_{\mathcal{D}}(h')\leq \varepsilon.
  \end{equation*}
  We can see that it holds for a given $\delta$ and $0 \leq \varepsilon_1 \leq \varepsilon_2 \leq 1$, that if\\ $L_{\mathcal{D}}(h) - \min_{h' \in \mathcal{H}} L_{\mathcal{D}}(h') \leq \varepsilon_1$,
  then it holds that\\ 
  $L_{\mathcal{D}}(h) - \min_{h' \in \mathcal{H}} L_{\mathcal{D}}(h')\leq \varepsilon_2$, and since $m_{\mathcal{H}}$ is the minimum number of examples in the sample for which this holds, 
  we can see that $m_{\mathcal{H}}(\epsilon_1, \delta) \geq m_{\mathcal{H}}(\epsilon_2, \delta)$.\\ 
  Also for a given $\varepsilon$ and $0\leq \delta_1 \leq \delta_2 \leq 0$ it holds that if\\ 
  $\P_{x\sim \mathcal{D}}(L_{\mathcal{D}}(h) - \min_{h' \in \mathcal{H}} L_{\mathcal{D}}(h') \leq \varepsilon) \geq 1-\delta_1$, then it holds that\\ 
  $\P_{x\sim \mathcal{D}}(L_{\mathcal{D}}(h) - \min_{h' \in \mathcal{H}} L_{\mathcal{D}}(h') \leq \varepsilon) \geq 1-\delta_2$, and because $m_{\mathcal{H}}$ is the minimum amount of examples in the sample for which this holds, we can see that $m_{\mathcal{H}}(\epsilon, \delta_1) \geq m_{\mathcal{H}}(\epsilon, \delta_2)$.\\

\TODO Please check \TODO
\end{solution}

\section*{Exercise 3.3}
Let $\mathcal{X} = \R^2$, $\mathcal{Y} = \paren*{0,1}$, and let $\mathcal{H}$ be the class of concentric circles in the plane, that is, $\mathcal{H}=\paren*{h_r:r \in \R_+}$, where $h_r(x) = \Ind_{\cbrac{x \in \R^2\c\nrm{x} \leq r}}$. Prove that $\mathcal{H}$ is PAC learnable (assume realizability), and that its sample complexity is bounded by
\begin{equation*}
  m_{\mathcal{H}}\leq \left\lceil \frac{\log(1/\delta )}{\varepsilon} \right\rceil .
\end{equation*}
\begin{solution}
%   Take $A$ to be an algorithm that returns an indicator function that points at the the smallest circle, enclosing all positive examples. Let $r(S)$ denote the radius of said circle.
%   Let $S|_x  = \mathbf{x}_1,\ldots \mathbf{x}_m$ with $\mathbf{x} \in \R^2$ be the sampled domain points in $S$. 
%   %Let $p$ denote the indices of the positive examples in $S|_x$, now $r(S) = \max_{k\in p}||x_k||  \leq r^* $. 
%   Let $r'$ be a radius such that
%   \begin{equation*}
%     \mathcal{D}(\{\mathbf{a} \in \R^2 : r'\leq||\mathbf{a}|| \leq r^*\}) = \varepsilon
%   \end{equation*}
%   Then it holds that if there exists a positive point $\mathbf{x} \in S|_x$, such that $r' \leq ||\mathbf{x}||\leq r^*$, then it holds that $L_{\mathcal{D},f}(h_S)\leq \varepsilon$.\\
% Note that if $\mathcal{D}(\{\mathbf{a}:||\mathbf{a}||-r^{*}\}) < \varepsilon $, there exists no $r'$, such that $\mathcal{D}(\{\mathbf{a}:r'\leq||\mathbf{a}|| \leq r^*\}) = \varepsilon$, but in that case the error will always be smaller than $\varepsilon$.\\
%   The probabilty that for a given point $\mathbf{x} \in S|_{x}$ in $r^*$, it holds that $r' \leq ||\mathbf{x}||\leq r^*$, equals $\varepsilon$. This means that the chance that there exists no such point is at most $(1-\varepsilon)^m \leq e^{-\varepsilon m}$. Let

% \begin{equation*}
%   m \geq \frac{\log(1/\delta)}{\varepsilon},
% \end{equation*}
% then we can see that $e^{-\varepsilon m} \leq e^{-\varepsilon \frac{\log(1/\delta)}{\varepsilon}} = \delta$. This means that we have with probability of at least $1- \delta$ that there exists such a point and thus that $A$ returns an hypothesis $h_S$ with error $L_{\mathcal{D},f}(h_S) \leq \varepsilon$. For this we needed a sample size $m \geq \frac{\log(1/\delta)}{\varepsilon}$, so we know that the minimum sample size for which $\mathcal{H}$ is PAC learnable has an upper bound, namely 
%   \begin{equation*}
%     m_{\mathcal{H}}\leq \left\lceil \frac{\log(1/\delta )}{\varepsilon} \right\rceil
%   \end{equation*}
%   \TODO Please check \TODO
  
%   Take $A$ to be an algorithm that returns an indicator function that points at the the smallest circle, enclosing all positive examples. Let $r(S)$ denote the radius of said circle.
%   Let $S|_x  = \mathbf{x}_1,\ldots \mathbf{x}_m$ with $\mathbf{x} \in \R^2$ be the sampled domain points in $S$. 
%   %Let $p$ denote the indices of the positive examples in $S|_x$, now $r(S) = \max_{k\in p}||x_k||  \leq r^* $. 
%   If $\mathcal{D}(\{\mathbf{a}:||\mathbf{a}||\leq r^{*}\}) \leq \varepsilon $, 
%   then the error will always at most $\varepsilon$.
%   So, we can assume this is not the case.
%   Let $r'$ be a radius such that
%   \begin{equation*}
%     \mathcal{D}(\{\mathbf{a} \in \R^2 : r' \leq \nrm{\mathbf{a}} \leq r^*\}) \leq \varepsilon, \text{ and }
%     \inf_{r<r'}\mathcal{D}(\{\mathbf{a} \in \R^2 : r \leq \nrm{\mathbf{a}} \leq r^*\}) \geq \varepsilon.
%   \end{equation*}
%   \TODO

    \TODO Please check \TODO
  The previous solution (commented out) doesn't work when $\mathcal{D}(\{\mathbf{x}\in $B$\})> \varepsilon$, with $B \subseteq \{\mathbf{a}: ||\mathbf{a}|| \leq r^*\}$ such that there exists no Lebesgue measure on it, because then it is possible that there doesn't exist a radius, $r'$ for which $\mathcal{D}(\{\mathbf{a}: ||a|| \leq r`\})=\varepsilon$. \\
  Maybe we should try something like: \\
  \TODO Please check: \\
  Take $A$ to be an algorithm that returns an indicator function that points at the the smallest circle, enclosing all positive examples. Let $r(S)$ denote the radius of said circle.
  Let $S|_x  = \mathbf{x}_1,\ldots \mathbf{x}_m$ with $\mathbf{x} \in \R^2$ be the sampled domain points in $S$. 
  %Let $p$ denote the indices of the positive examples in $S|_x$, now $r(S) = \max_{k\in p}||x_k||  \leq r^* $. 
  If $\mathcal{D}(\{\mathbf{a}:||\mathbf{a}||\leq r^{*}\}) \leq \varepsilon $, 
  then the error will always at most $\varepsilon$ and then we're already done.\\
  So, without loss of generality, we can assume this is not the case.
  Let $r' \leq r^*$ be a radius such that
  \begin{equation*}
   r' = \inf_{r<r*} \{ r\in \R: \mathcal{D}(\{ \mathbf{a} \in \R^2: r \leq \nrm{\mathbf{a}} \leq r^*\}) < \varepsilon \}.
 \end{equation*}
  Now let $C$ be annulus between the circle with radius $r'$ and the circle with radius $r^*$, i.e.:
  \begin{equation*}
    C = \{\mathbf{a} \in \R^2: r' \leq \nrm{\mathbf{a}} \leq r^* \}
  \end{equation*}
  We can see that if there exists an example $\mathbf{x} \in S|_x$ such that $r'\leq \nrm{x} \leq r^*$, then the error is upper bounded by $\varepsilon$.\\
  We have that by definition of $r'$ that
  \begin{equation*}
    \mathcal{D}(\mathbf{a} \in \R^2: r' \leq \nrm{\mathbf{a}}\leq r*) = \mathcal{D}(C) \geq \varepsilon
  \end{equation*}
  We see that probability of a given point $\mathbf{x} \in S|_x$, that is in $C$ is at least $\varepsilon$. This means that the probability that none of the examples in $S|_x$ is in $C$, is at most $(1-\varepsilon)^m \leq e^{-\varepsilon m}$. Let

\begin{equation*}
  m \geq \frac{\log(1/\delta)}{\varepsilon},
\end{equation*}
then we can see that $e^{-\varepsilon m} \leq e^{-\varepsilon \frac{\log(1/\delta)}{\varepsilon}} = \delta$. This means that we have with probability of at least $1- \delta$ that there exists such a point and thus that $A$ returns an hypothesis $h_S$ with error $L_{\mathcal{D},f}(h_S) \leq \varepsilon$. For this we needed a sample size $m \geq \frac{\log(1/\delta)}{\varepsilon}$, so we know that the minimum sample size for which $\mathcal{H}$ is PAC learnable has an upper bound, namely 
  \begin{equation*}
    m_{\mathcal{H}}\leq \left\lceil \frac{\log(1/\delta )}{\varepsilon} \right\rceil
  \end{equation*}
\end{solution}


\section*{Exercise 3.6}
Let $\mathcal{H}$ be a hypothesis class of binary classifiers. Show that if $\mathcal{H}$ is agnostic PAC learnable, then $\mathcal{H}$ is PAC learnable as well.
Furthermore, if $A$ is a successful agnostic PAC learner for $\mathcal{H}$, then $A$ is also a successful PAC learner for $\mathcal{H}$.
\begin{solution}
  A hypothesis class is agnostic agnostic  PAC learnable, it means that by definition there exists a function $m_{\mathcal{H}}:(0,1)^2\to \N$ and an algorithm, $A$, with the following property: For every $\varepsilon, \delta \in (0,1)$ and for every distribution $\mathcal{D}$ over $\mathcal{X} \times \mathcal{Y}$, when running the learning algorithm on $m\geq m_{\mathcal{H}}$ \textit{i.i.d.} examples, generated by $\mathcal{D}$, the algorithm returns a hypothesis $h$ for which with probability of at least $1-\delta$ (over the choice of $m$ training examples),
  \begin{equation*}
    L_{\mathcal{D}}(h) \leq \min_{h' \in \mathcal{H}}L_{\mathcal{D}}(h')+\varepsilon
  \end{equation*}
  In the case of PAC learning, we can do the realizability assumption and assume that there exists an $h^* \in \mathcal{H}$ such that $L_{(\mathcal{D},f)}(h^*) = 0$. Since for any $h \in \mathcal{H}$ it holds that $L_{D,f}(h) \geq 0$, we can see that
  \begin{equation*}
    L_{(D,f)}(h^*) = \min_{h' \in \mathcal{H}}L_{\mathcal{D}}(h') =0.
  \end{equation*}
  So if $\mathcal{H}$ is agnostic PAC learnable, and algorithm $A$ returns a successful hypothesis $h$, and we can do the realizability assumption, then, with probability of at least $1-\delta$ it holds that
  \begin{align*}
    L_{\mathcal{D}}(h) &\leq \min_{h' \in \mathcal{H}}L_{\mathcal{D}}(h')+\varepsilon \\
    \iff L_{\mathcal{D}}(h) &\leq  L_{(D,f)}(h^*) + \varepsilon \\
    \iff L_{\mathcal{D}}(h) &\leq  \varepsilon
  \end{align*}
  This by definition means that $\mathcal{H}$ is PAC learnable, and $A$ is a successful PAC learner.
    
  \TODO Please check, because this seems to hold for any $\mathcal{H}$, but in the question they specifically say binary classifiers, or is that because they have $\varepsilon, \delta \in (0,1)$ in the definition? \TODO
\end{solution}


\section*{Exercise 3.7}
\TODO I added explanations on this question, can you check if those are correct? The I know I understood everything correctly.\\

\textbf{The Bayes optimal predictor:} Show that for every probability distribution $\mathcal{D}$, the Bayes optimal predictor $f_{\mathcal{D}}$ is optimal, in the sense that for every classifier $g$ from $\mathcal{X}$ to $\paren*{0,1}$, $L_{\mathcal{D}}(f_{\mathcal{D}}) \leq L_{\mathcal{D}}(g)$.
\begin{solution}
Let $g:\mathcal{X} \to \cbrac{0,1}$ be given.
Let $A=\cbrac{x \in \mathcal{X} \c f_{\mathcal{D}}(x)=1}$ and $B=\cbrac{x \in \mathcal{X} \c g(x)=1}$.
By the definition of $f_{\mathcal{D}}$, we then have for all $U\se A$ that
\begin{align*}
 \mathcal{D}(U\times\cbrac{1}) & \geq \tfrac{1}{2}\mathcal{D}(U \times \cbrac{0,1}) \\
 & \geq \mathcal{D}(U\times\cbrac{0})
\end{align*}
and for all $W \se A^C$ with $W \neq \varnothing$ that
\begin{align*}
 \mathcal{D}(W\times\cbrac{0}) &> \tfrac{1}{2}\mathcal{D}(W \times \cbrac{0,1}) \\
 &> \mathcal{D}(W\times\cbrac{1}).
\end{align*}
We then have that
\begin{align*}
&L_{\mathcal{D}}(g)-L_{\mathcal{D}}(f_{\mathcal{D}})\\
 &= \mathcal{D}(\cbrac{(x,y)\in \mathcal{X}\times\cbrac{0,1} \c g(x)\neq y})- 
   \mathcal{D}(\cbrac{(x,y)\in \mathcal{X}\times\cbrac{0,1} \c f_{\mathcal{D}}(x)\neq y}) \\
&= \mathcal{D}(B\times\cbrac{0})+\mathcal{D}(B^C\times\cbrac{1}) - 
 \mathcal{D}(A\times\cbrac{0})-\mathcal{D}(A^C\times\cbrac{1}) \\
\end{align*}
We can see that $\mathcal{D}$ over instances in $\mathcal{X}$ where $f_{\mathcal{D}}$ and $g$ are both assign the wrong label are annihilated in this equation so we have that
\begin{align*}
  &L_{\mathcal{D}}(g)-L_{\mathcal{D}}(f_{\mathcal{D}})\\
 &=  \mathcal{D}((B\setminus A)\times\cbrac{0})+ \mathcal{D}((B^C\setminus A^C)\times\cbrac{1})
- \mathcal{D}((A\setminus B)\times\cbrac{0})- \mathcal{D}((A^C\setminus B^C)\times\cbrac{1}) \\
&=  \mathcal{D}((B\setminus A)\times\cbrac{0})+ \mathcal{D}((A\setminus B)\times\cbrac{1})
- \mathcal{D}((A\setminus B)\times\cbrac{0})- \mathcal{D}((B\setminus A)\times\cbrac{1}).
\end{align*}
As $(B\setminus A) \se A^C$ and $(A\setminus B) \se A$ we have that
\begin{equation*}
\mathcal{D}((B\setminus A)\times\cbrac{0}) \geq \mathcal{D}((B\setminus A)\times\cbrac{1})  
\end{equation*}
and 
\begin{equation*}
\mathcal{D}((A\setminus B)\times\cbrac{1}) \geq \mathcal{D}((A\setminus B)\times\cbrac{0}).  
\end{equation*}
It follows that
\begin{align*}
 L_{\mathcal{D}}(g)-L_{\mathcal{D}}(f_{\mathcal{D}}) \geq 0.
\end{align*}
\end{solution}


\section*{Exercise 4.1}
In this exercise, we show that the $(\varepsilon , \delta )$ requirement on the convergence of errors in our definition of PAC learning, is, in fact, quite close to a simpler looking requirement about averages (or expectations). Prove that the following two statements are equivalent (for any learning algorithm $A$, any probability distribution $\mathcal{D}$, and any loss function whose range is $[0,1]$):
\begin{enumerate}
 \item \label{it:convergence_probability} For every $\varepsilon , \delta > 0$, there exists $m(\varepsilon , \delta)$ such that $\forall m \leq m(\varepsilon , \delta)$
\begin{equation*}
\P_{S \sim \mathcal{D}^m}(L_{\mathcal{D}}(A(S))>\varepsilon)<\delta.
\end{equation*}
\item \label{it:convergence_expectation} 
\begin{equation*}
\lim_{m\to \infty}\Ev_{S\sim \mathcal{D}^{m}}(L_{\mathcal{D}}(A(S))) = 0.
\end{equation*}
\end{enumerate}
\begin{solution}
Consider the measure space $((\mathcal{X}\times\mathcal{Y})^\infty, \Fa^{\infty}, \mathcal{D}^\infty)$.
For each sequence $S=(x_i,y_i)_{i \in \N} \in (\mathcal{X}\times\mathcal{Y})^\infty$
let $S_m=(x_i,y_i)_{i=1}^{m}$ denote the truncated sequence of length $m$.
Define for each $m \in \N$ the random variable $L_m:(\mathcal{X}\times\mathcal{Y})^\infty \to [0,1]$ by
\begin{equation*}
 L_m(S)=L_{\mathcal{D}}(A(S_m)).
\end{equation*}

First assume that statement (\ref{it:convergence_expectation}) holds. 
Note that we then have that 
\begin{align*}
 \lim_{m\to \infty}\Ev_{S\sim \mathcal{D}^{m}}(L_{\mathcal{D}}(A(S))) &= 
 \lim_{m\to \infty}\Ev_{S\sim \mathcal{D}^{\infty}}(L_{\mathcal{D}}(A(S_m))) \\
 & = \lim_{m\to \infty}\Ev_{S\sim \mathcal{D}^{\infty}}(L_m(S)).
\end{align*}
This means that the sequence $(L_m)_{m \in \N}$ converges in $L^1$-norm to $0$.
For any sequence of random variables convergence in $L^1$ implies convergence in probability, this gives us that
\begin{equation*}
 \lim_{m\to \infty}\mathcal{D}^{\infty}(L_m>\varepsilon)=0.
\end{equation*}
From this statement (\ref{it:convergence_probability}) follows.

For almost surely bounded sequences of random variables, we have that convergence in probability also implies convergence in 
$L^1$. Since $(L_m)_{m\in \N}$ takes values in $[0,1]$ it is surely bounded, hence $L_m \stackrel{\mathcal{D}^{\infty}}{\to} 0$ implies that $L_m \stackrel{L^1}{\to} 0$. So, if we assume that (\ref{it:convergence_probability}) holds, then (\ref{it:convergence_expectation}) holds as well.
\end{solution}

\section*{Exercise 4.2}
\textbf{Bounded loss functions:}
In corollary 4.6 we assumed that the range of the loss function is $[0,1]$. Prove that if the range of the loss function is $[a,b]$ then the sample complexity satisfies
\begin{equation*}
  m_{\mathcal{H}}(\epsilon , \delta) \leq m_{\mathcal{H}}^{\text{UC}}(\epsilon /2, \delta)
  \leq \ceil*{\frac{2\log (2|\mathcal{H}|/\delta) (b-a)^2}{\epsilon^2}} .
\end{equation*}
\begin{solution}
Note that $L_S(h)=\tfrac{1}{m}\sum_{z \in S}\ell(h,z)$ and $L_{\mathcal{D}}(h)=\Ev_{S \sim \mathcal{D}^m}(L_S(h))$.
Denote $\mu_h=\Ev_{z \sim \mathcal{D}}(\ell(z, h))$, then we also have that $\mu_h=L_{\mathcal{D}}(h)$.
Hence, by Hoeffding's inequality, we have for all $\varepsilon > 0$ that
\begin{align*}
  \mathcal{D}^m(\cbrac{S \text{ is not }\tfrac{\varepsilon}{2}-\text{representative}}) 
  &=\mathcal{D}^{m}\paren*{\bigcup_{h \in \mathcal{H}}\abs*{\tfrac{1}{m}\sum_{z \in S}\ell(h,z)-\mu_h}>\tfrac{\varepsilon}{2}} \\
  &\leq \sum_{h \in \mathcal{H}}\mathcal{D}^{m}\paren*{\abs*{\tfrac{1}{m}\sum_{z \in S}\ell(h,z)-\mu_h}>\tfrac{\varepsilon}{2}} \\
  &\leq \sum_{h \in \mathcal{H}} 2e^{-\frac{m\varepsilon^2}{2(b-a)^2}} \\
  &= \abs{\mathcal{H}} 2e^{-\frac{m\varepsilon^2}{2(b-a)^2}}.
  \end{align*}

 For each $m \geq  \frac{2(b-a)^2}{\varepsilon^2}\log(\frac{2\abs{H}}{\delta})$ it holds that
 $\abs{\mathcal{H}} 2e^{-\frac{m\varepsilon^2}{2(b-a)^2}} \leq \delta$.
 Hence we have that 
 \begin{equation*}
 m_{\mathcal{H}}^{UC}(\tfrac{\varepsilon}{2},\delta) \leq \ceil*{\frac{2(b-a)^2}{\varepsilon^2}\log\paren*{\frac{2\abs{H}}{\delta}}}.
 \end{equation*}

 
  
% \begin{align*}
%  \abs{H}2e^{-\frac{m\varepsilon^2}{2(b-a)^2}}&=\delta \\
%  e^{-\frac{m\varepsilon^2}{2(b-a)^2}}&=\frac{\delta}{2\abs{H}} \\
%  -\frac{m\varepsilon^2}{2(b-a)^2}&=\log(\frac{\delta}{2\abs{H}}) \\
%  \frac{m\varepsilon^2}{2(b-a)^2}&=\log(\frac{2\abs{H}}{\delta}) \\
%  m&=\frac{2(b-a)^2}{\varepsilon^2}\log(\frac{2\abs{H}}{\delta}).
% \end{align*}
\end{solution}

\section*{Exercise 5}
Prove that when the expected losses $L_{\mathcal{D}}(h)$ are bounded, we have
\begin{equation*}
  L_{\mathcal{D}}(h_S) - \inf_{h\in \mathcal{H}}L_{\mathcal{D}}(h)
  \leq 2 \sup_{h\in \mathcal{H}}\abs{L_S(h)-L_{\mathcal{D}}(h)}.
\end{equation*}
\begin{solution}
We assume that $A:\bigcup_{m=1}^{\infty}(\mathcal{X}\times\mathcal{Y})^m \to \mathcal{H}, S \mapsto h_S$
is an $\mathrm{ERM}_{\mathcal{H}}$ learner.

Denote $\varepsilon = \sup_{h\in \mathcal{H}}\abs{L_S(h)-L_{\mathcal{D}}(h)}$.
Then by definition the training set $S$ is $\varepsilon$-representative.
Hence, the statement follows by applying lemma 4.2.


% \TODO Show statement does not hold without ERM assumption.
%     Let $\mathcal{X}=\cbrac{0,1}$ and $\mathcal{Y}=\cbrac{0,1}$. 
%     Define the probability measure $\mathcal{D}$ on $\mathcal{X}\times \mathcal{Y}$ by $\mathcal{D}(\cbrac{(1,1)})=1$.
%     Consider the hypothesis class
%     \begin{equation*}
%      \mathcal{H}=\cbrac{h_1(x)=1, h_0(x)=0}
%     \end{equation*}
%     consisting of the two constant functions from $\mathcal{X}\to\mathcal{Y}$.
%     Let the loss function be given by
%     \begin{equation*}
%      L_{\mathcal{D}}(h)=\D(\cbrac{x,y} \in \mathcal{X}\times \mathcal{Y} \c y=h(x))
%     \end{equation*}
%     and the sample loss by
%     \begin{equation*}
%      L_{S}(h)=\tfrac{1}{m}\abs{\cbrac{i \in [m] \c h(x_i)=y_i}},
%     \end{equation*}
%     where $m=\abs{S}$. 
%     \TODO
\end{solution}
\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
