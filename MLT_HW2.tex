\documentclass[10pt, a4paper, twoside]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
%\usepackage[utf8x]{inputenc}
\usepackage{amsmath, amssymb, amsthm, amsfonts, mathrsfs, amsfonts}
\usepackage{mathtools} 

\usepackage{enumerate}

\usepackage[noabbrev]{cleveref}

%\usepackage{subfig}
\usepackage{pgf,tikz}
%\usetikzlibrary{arrows}

%\usepackage{natbib}
\usepackage[osf]{mathpazo} % Nice text font
\usepackage{euler} % Very nice mathmode font


% bold
\newcommand{\F}{\ensuremath{\mathbb{F}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
% 
% calligraphic
\newcommand{\A}{\ensuremath{\mathcal{A}}}
\newcommand{\E}{\ensuremath{\mathcal{E}}}
\newcommand{\I}{\ensuremath{\mathcal{I}}}

% Delimiters (requires mathtools package)
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\brac[]
\DeclarePairedDelimiter\cbrac\{\}
\DeclarePairedDelimiter\paren()
\DeclarePairedDelimiter{\ip}\langle\rangle
\DeclarePairedDelimiter{\nrm}\lVert\rVert

% Power set
\newcommand{\Ps}{\ensuremath{\mathcal{P}}}

 
% other things ...
\renewcommand{\c}{\ensuremath{\colon}}
\newcommand{\se}{\ensuremath{\subseteq}}
\renewcommand{\d}{\ensuremath{\ d}}
\newcommand{\Ind}{\ensuremath{\mathbb{1}}}
\newcommand{\im}{\ensuremath{\mathbf{i}}}

\newcommand{\argmin}{\operatorname*{argmin}}
\newcommand{\argmax}{\operatorname*{argmax}}
% \renewcommand\qedsymbol{\rule{1ex}{1ex}}

% course specifics
\renewcommand{\P}{\operatorname*{\ensuremath{\mathbf{P}}}} % probability measure
\newcommand{\Ev}{\operatorname*{\ensuremath{\mathbf{E}}}} %expected value
\newcommand{\Fa}{\ensuremath{\mathcal{F}}} %sigma-algebra

\newcommand{\sgn}{\mathrm{sgn}}

% Solutions use a modified proof environment
\newenvironment{solution}
               {\let\oldqedsymbol=\qedsymbol
                \renewcommand{\qedsymbol}{$\blacktriangleleft$}
                \begin{proof}[Solution]}
               {\end{proof}
                \renewcommand{\qedsymbol}{\oldqedsymbol}}


\newcommand{\TODO}{\textcolor{red}{\textbf{!!!!!! }}}

\newcommand{\firstName}  {Twan}
\newcommand{\lastName}   {Koperberg}
\newcommand{\studId}     {0713309 (Leiden)}
\renewcommand{\email}    {twankop@gmail.com}

\newcommand{\firstNameII}  {Tamar}
\newcommand{\lastNameII}   {Huygen}
\newcommand{\studIdII}     {10907483 (UvA)}
\newcommand{\studIdIII}    {2556474 (VU)}
\newcommand{\emailII}     {tamar@huygen.nl}

\begin{document}
\begin{center}

  {\huge\bf Measure learning theory}\\
  {\large\sc Homeworkset 2 }\\ \vspace{1em}
  \firstName \textsc{ \lastName}, {\sc s}\studId \\
  \email\text{}\\ \smallskip
  \firstNameII \textsc{ \lastNameII}, \studIdII, \studIdIII\\
  \emailII \\ \bigskip
  \today \\\bigskip
  \hrule
  \bigskip
\end{center}


\section*{Exercise 3.1} 
Let $\mathcal{H}$ be a hypothesis class for a binary classification task. Suppose that $\mathcal{H}$ is PAC learnable and its sample complexity given by $m_{\mathcal{H}}(\cdot,\cdot)$.
Show that $m_{\mathcal{H}}$ is monotonically nonincreasing in each of its parameters. That is, show that given $\delta \in (0,1)$, and given $0<\varepsilon_1 \leq \varepsilon_2<1$, we have that $m_{\mathcal{H}}(\varepsilon_1, \delta) \geq m_{\mathcal{H}}(\varepsilon_2, \delta)$. Similarly, show that given $\varepsilon \in (0,1)$, and given $0<\delta_1 \leq \delta_2 <1$, we have that $m_{\mathcal{H}}(\varepsilon, \delta_1) \geq m_{\mathcal{H}}(\varepsilon, \delta_2)$.
\begin{solution}
\TODO Doet Tamar
  Notes:
Intu\"itively this is immediately clear; the less examples there are in the sample, the smaller the probability that the learner has a given accuracy, or with a given probability have smaller accuracy.
  
  We know that 
  \begin{equation*}
    \Ev_{S}[L_S] = L_{\mathcal{D},f}
  \end{equation*}
  
  When the sample is $\epsilon$-Representative, it holds that
  \begin{equation*}
    L_S(h) \leq L_{\mathcal{D},f} +\epsilon
  \end{equation*}
  Then it holds that
  \begin{equation*}
    L_S(h) \leq L_{\mathcal{D},f} +\epsilon \leq 2\epsilon
  \end{equation*}
  By the definition of $L_S(h)$ we can see that
  \begin{equation*}
    L_S(h) = \frac{|\{i \in[m]:h(x_i)\neq y_i \}|}{m} \leq 2\epsilon
  \end{equation*}
  We can see that $m$ 
  Now we have that $m$ is inverse proportional to $\epsilon$
  \begin{equation*}
f    
  \end{equation*}
  
  
% Let $h_S:\R^d\to\cbrac{0,1}$ be given by
% \begin{equation*}
%  h_S(x)=
%  \begin{cases}
%   f(x_i) & \text{ if there exists }i \in [m] \text{ such that } x=x_i, \\
%   0 & \text{ otherwise.} \\  
%  \end{cases}
% \end{equation*}

% Consider the polynomial $p_S:\R^d \to \R$ given by
% \begin{equation*}
%  p_S(x^1,x^2,\ldots,x^d)=-\prod_{i=1}^{m}\paren*{(1-f(x_i))+\sum_{j=1}^d(x^j-x_i^j)^2},
% \end{equation*}
% where $x^j$ denotes the $j$-th component of $x \in \R^d$.

% Note that $(1-f(x_i))+\sum_{j=1}^d(x^j-x_i^j)^2 \geq 0$ for all $i \in [m]$.
% This means that $p_{S}(x)\leq 0$ for all $x \in \R^d$. Also we have that $p_S(x)=0$ if and only if there exists an 
% $x_i \in S|_x$ such that $x=x_i$ and $f(x_i)=1$. So, we have that $p_S(x)=0$ if and only if $h_S(x)=1$.

\end{solution}

\section*{Exercise 3.3}
Let $\mathcal{X} = \R^2$, $\mathcal{Y} = \paren*{0,1}$, and let $\mathcal{H}$ be the class of concentric circles in the plane, that is, $\mathcal{H}=\paren*{h_r:r \in \R_+}$, where $h_r(x) = \Ind_{\cbrac{x \in \R^2\c\nrm{x} \leq r}}$. Prove that $\mathcal{H}$ is PAC learnable (assume realizability), and that its sample complexity is bounded by
\begin{equation*}
  m_{\mathcal{H}}\leq \left\lceil \frac{\log(1/\delta )}{\epsilon} \right\rceil .
\end{equation*}
\begin{solution}
  Take $A$ to be an algorithm that returns an indicator function that points at the the smallest circle, enclosing all positive examples. Let $r(S)$ denote the radius of said circle.
  Let $S|_x  = \mathbf{x}_1,\ldots \mathbf{x}_m$ with $\mathbf{x} \in \R^2$ be the sampled domain points in $S$. 
  %Let $p$ denote the indices of the positive examples in $S|_x$, now $r(S) = \max_{k\in p}||x_k||  \leq r^* $. 
  Let $r'$ be a radius such that
  \begin{equation*}
    \mathcal{D}(\{\mathbf{a} \in \R^2 : r'\leq||\mathbf{a}|| \leq r^*\}) = \epsilon
  \end{equation*}
  Then it holds that if there exists a positive point $\mathbf{x} \in S|_x$, such that $r' \leq ||\mathbf{x}||\leq r^*$, then it holds that $L_{\mathcal{D},f}(h_S)\leq \epsilon$.\\
Note that if $\mathcal{D}(\{\mathbf{a}:||\mathbf{a}||-r^{*}\}) < \epsilon $, there exists no $r'$, such that $\mathcal{D}(\{\mathbf{a}:r'\leq||\mathbf{a}|| \leq r^*\}) = \epsilon$, but in that case the error will always be smaller than $\epsilon$.\\
  The probabilty that for a given point $\mathbf{x} \in S|_{x}$ in $r^*$, it holds that $r' \leq ||\mathbf{x}||\leq r^*$, is $\epsilon$. This means that the chance that there exists no such point is at most $(1-\epsilon)^m \leq e^{-\epsilon m}$. Let

\begin{equation*}
  m \geq \frac{\log(1/\delta)}{\epsilon},
\end{equation*}
then we can see that $e^{-\epsilon m} \leq e^{-\epsilon \frac{\log(1/\delta)}{\epsilon}} = \delta$. This means that we have with probability of at least $1- \delta$ that there exists such a point and thus that $A$ returns an hypothesis $h_S$ with error $L_{\mathcal{D},f}(h_S) \leq \epsilon$. For this we needed a sample size $m \geq \frac{\log(1/\delta)}{\epsilon}$, so we know that the minimum sample size for which $\mathcal{H}$ is PAC learnable has an upper bound, namely 
  \begin{equation*}
    m_{\mathcal{H}}\leq \left\lceil \frac{\log(1/\delta )}{\epsilon} \right\rceil
  \end{equation*}
  \TODO Please check \TODO
  
  Take $A$ to be an algorithm that returns an indicator function that points at the the smallest circle, enclosing all positive examples. Let $r(S)$ denote the radius of said circle.
  Let $S|_x  = \mathbf{x}_1,\ldots \mathbf{x}_m$ with $\mathbf{x} \in \R^2$ be the sampled domain points in $S$. 
  %Let $p$ denote the indices of the positive examples in $S|_x$, now $r(S) = \max_{k\in p}||x_k||  \leq r^* $. 
  If $\mathcal{D}(\{\mathbf{a}:||\mathbf{a}||\leq r^{*}\}) \leq \varepsilon $, 
  then the error will always at most $\varepsilon$.
  So, we can assume this is not the case.
  Let $r'$ be a radius such that
  \begin{equation*}
    \mathcal{D}(\{\mathbf{a} \in \R^2 : r' \leq \nrm{\mathbf{a}} \leq r^*\}) \leq \varepsilon, \text{ and }
    \inf_{r<r'}\mathcal{D}(\{\mathbf{a} \in \R^2 : r \leq \nrm{\mathbf{a}} \leq r^*\}) \geq \varepsilon.
  \end{equation*}
  \TODO
  
\end{solution}


\section*{Exercise 3.6}
Let $\mathcal{H}$ be a hypothesis class of binary classifiers. Show that if $\mathcal{H}$ is agnostic PAC learnable, then $\mathcal{H}$ is PAC learnable as well.
Furthermore, if $A$ is a successful agnostic PAC learner for $\mathcal{H}$, then $A$ is also a successful PAC learner for $\mathcal{H}$.
\begin{solution}
  When a hypothesis class is agnostic agnostic  PAC learnable, it means that by definition there exists a function $m_{\mathcal{H}}:(0,1)^2\to \N$ and an algorithm, $A$, that when running on $m\geq m_{H}$ \textit{i.i.d.} examples, generated by $\mathcal{D}$, returns a hypothesis for which
  with probability $1-\delta$
  \begin{equation*}
    L_{\mathcal{D}(h)} \leq \min_{h' \in \mathcal{H}}L_{\mathcal{D}}(h')+\epsilon
  \end{equation*}
In the case of PAC learning, we can do the realizability assumption and assume that there exists an $h^* \in \mathcal{H}$ such that $L_{D,f}(h^*) = 0$. In the case of PAC learnability, 
  
    
  \TODO Doet Tamar
\end{solution}


\section*{Exercise 3.7}
\textbf{The Bayes optimal predictor:} Show that for every probability distribution $\mathcal{D}$, the Bayes optimal predictor $f_{\mathcal{D}}$ is optimal, in the sense that for every classifier $g$ from $\mathcal{X}$ to $\paren*{0,1}$, $L_{\mathcal{D}}(f_{\mathcal{D}}) \leq L_{\mathcal{D}}(g)$.
\begin{solution}
Let $g:\mathcal{X} \to \cbrac{0,1}$ be given.
Let $A=\cbrac{x \in \mathcal{X} \c f_{\mathcal{D}}(x)=1}$ and $B=\cbrac{x \in \mathcal{X} \c g(x)=1}$.
By the definition of $f_{\mathcal{D}}$, we then have for all $U\se A$ that
\begin{align*}
 \mathcal{D}(U\times\cbrac{1}) & \geq \tfrac{1}{2}\mathcal{D}(U \times \cbrac{0,1}) \\
 & \geq \mathcal{D}(U\times\cbrac{0})
\end{align*}
and for all $W \se A^C$ with $W \neq \varnothing$ that
\begin{align*}
 \mathcal{D}(W\times\cbrac{0}) &> \tfrac{1}{2}\mathcal{D}(W \times \cbrac{0,1}) \\
 &> \mathcal{D}(W\times\cbrac{1}).
\end{align*}
We then have that
\begin{align*}
 L_{\mathcal{D}}(g)-L_{\mathcal{D}}(f_{\mathcal{D}}) &=
 \mathcal{D}(\cbrac{(x,y)\in \mathcal{X}\times\cbrac{0,1} \c g(x)\neq y})- 
 \mathcal{D}(\cbrac{(x,y)\in \mathcal{X}\times\cbrac{0,1} \c f_{\mathcal{D}}(x)\neq y}) \\
 &= \mathcal{D}(B\times\cbrac{0})+\mathcal{D}(B^C\times\cbrac{1}) - 
 \mathcal{D}(A\times\cbrac{0})-\mathcal{D}(A^C\times\cbrac{1}) \\
 &=  \mathcal{D}((B\setminus A)\times\cbrac{0})+ \mathcal{D}((B^C\setminus A^C)\times\cbrac{1})
- \mathcal{D}((A\setminus B)\times\cbrac{0})- \mathcal{D}((A^C\setminus B^C)\times\cbrac{1}) \\
&=  \mathcal{D}((B\setminus A)\times\cbrac{0})+ \mathcal{D}((A\setminus B)\times\cbrac{1})
- \mathcal{D}((A\setminus B)\times\cbrac{0})- \mathcal{D}((B\setminus A)\times\cbrac{1}).
\end{align*}
As $(B\setminus A) \se A^C$ and $(A\setminus B) \se A$ we have that
\begin{equation*}
\mathcal{D}((B\setminus A)\times\cbrac{0}) \geq \mathcal{D}((B\setminus A)\times\cbrac{1})  
\end{equation*}
and 
\begin{equation*}
\mathcal{D}((A\setminus B)\times\cbrac{1}) \geq \mathcal{D}((A\setminus B)\times\cbrac{0}).  
\end{equation*}
It follows that
\begin{align*}
 L_{\mathcal{D}}(g)-L_{\mathcal{D}}(f_{\mathcal{D}}) \geq 0.
\end{align*}
\end{solution}


\section*{Exercise 4.1}
In this exercise, we show that the $(\epsilon , \delta )$ requirement on the convergence of errors in our definition of PAC learning, is, in fact, quite close to a simpler looking requirement about averages (or expectations). Prove that the following two statements are equivalent (for any learning algorithm $A$, any probability distribution $\mathcal{D}$, and any loss function whose range is $[0,1]$):
\begin{enumerate}
 \item \label{it:convergence_probability} For every $\epsilon , \delta > 0$, there exists $m(\epsilon , \delta)$ such that $\forall m \leq m(\epsilon , \delta)$
\begin{equation*}
\P_{S \sim \mathcal{D}^m}(L_{\mathcal{D}}(A(S))>\epsilon)<\delta.
\end{equation*}
\item \label{it:convergence_expectation} \begin{equation*}
\lim_{m\to \infty}\Ev_{S\sim \mathcal{D}^{m}}(L_{\mathcal{D}}(A(S))) = 0.
\end{equation*}
\end{enumerate}
\begin{solution}
Consider the measure space $((\mathcal{X}\times\mathcal{Y})^\infty, \Fa^{\infty}, \mathcal{D}^\infty)$.
For each sequence $S=(x_i,y_i)_{i \in \N} \in (\mathcal{X}\times\mathcal{Y})^\infty$
let $S_m=(x_i,y_i)_{i=1}^{m}$ denote the truncated sequence of length $m$.
Define for each $m \in \N$ the random variable $L_m:(\mathcal{X}\times\mathcal{Y})^\infty \to [0,1]$ by
\begin{equation*}
 L_m(S)=L_{\mathcal{D}}(A(S_m)).
\end{equation*}

First assume that statement (\ref{it:convergence_expectation}) holds. 
Note that we then have that 
\begin{align*}
 \lim_{m\to \infty}\Ev_{S\sim \mathcal{D}^{m}}(L_{\mathcal{D}}(A(S))) &= 
 \lim_{m\to \infty}\Ev_{S\sim \mathcal{D}^{\infty}}(L_{\mathcal{D}}(A(S_m))) \\
 & = \lim_{m\to \infty}\Ev_{S\sim \mathcal{D}^{\infty}}(L_m(S)).
\end{align*}
This means that the sequence $(L_m)_{m \in \N}$ converges in $L^1$-norm to $0$.
For any sequence of random variables convergence in $L^1$ implies convergence in probability, this gives us that
\begin{equation*}
 \lim_{m\to \infty}\mathcal{D}^{\infty}(L_m>\varepsilon)=0.
\end{equation*}
From this statement (\ref{it:convergence_probability}) follows.

For almost surely bounded sequences of random variables, we have that convergence in probability also implies convergence in 
$L^1$. Since $(L_m)_{m\in \N}$ takes values in $[0,1]$ it is surely bounded, hence $L_m \stackrel{\mathcal{D}^{\infty}}{\to} 0$ implies that $L_m \stackrel{L^1}{\to} 0$. So, if we assume that (\ref{it:convergence_probability}) holds, then (\ref{it:convergence_expectation}) holds as well.
\end{solution}

\section*{Exercise 4.2}
\textbf{Bounded loss functions:}
In corollary 4.6 we assumed that the range of the loss function is $[0,1]$. Prove that if the range of the loss function is $[a,b]$ then the sample complexity satisfies
\begin{equation*}
  m_{\mathcal{H}}(\epsilon , \delta) \leq m_{\mathcal{H}}^{\text{UC}}(\epsilon /2, \delta)
  \leq \left\lceil \frac{2\log (2|\mathcal{H}|/\delta) (b-a)^2}{\epsilon^2} \right\rceil .
\end{equation*}
\begin{solution}
  \TODO Doet Twan
\end{solution}

\section*{Exercise 5}
Prove that when the expected losses $L_{\mathcal{D}}(h)$ are bounded, we have
\begin{equation*}
  L_{\mathcal{D}}(h_S) - \inf_{h\in \mathcal{H}}L_{\mathcal{D}}(h)
  \leq 2 \sup_{h\in \mathcal{H}}|L_S(h)-L_{\mathcal{D}}(h)|.
\end{equation*}
\begin{solution}
    \TODO Doet Twan
\end{solution}
\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
