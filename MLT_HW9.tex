%%% Local Variables:
%%% mode: latex
%%% TeX-master: t

\documentclass[10pt, a4paper, twoside]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
%\usepackage[utf8x]{inputenc}
\usepackage{amsmath, amssymb, amsthm, amsfonts, mathrsfs, amsfonts}
\usepackage{mathtools} 

\usepackage{enumerate}

\usepackage[noabbrev]{cleveref}

%\usepackage{subfig}
\usepackage{pgf,tikz}
%\usetikzlibrary{arrows}

%\usepackage{natbib}
\usepackage[osf]{mathpazo} % Nice text font
\usepackage{euler} % Very nice mathmode font
\usepackage[many]{tcolorbox}    



\theoremstyle{plain}
\newtheorem{lemma}{Lemma}

% bold
\newcommand{\F}{\ensuremath{\mathbb{F}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
% 

% calligraphic
\newcommand{\A}{\ensuremath{\mathcal{A}}}
\newcommand{\I}{\ensuremath{\mathcal{I}}}

% Delimiters (requires mathtools package)
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\brac[]
\DeclarePairedDelimiter\cbrac\{\}
\DeclarePairedDelimiter\paren()
\DeclarePairedDelimiter{\ip}\langle\rangle
\DeclarePairedDelimiter{\nrm}\lVert\rVert
\DeclarePairedDelimiter{\ceil}\lceil\rceil

% Power set
\newcommand{\Ps}{\ensuremath{\mathcal{P}}}

 
% other things ...
\renewcommand{\c}{\ensuremath{\colon}}
\newcommand{\se}{\ensuremath{\subseteq}}
\renewcommand{\d}{\ensuremath{\ d}}
\newcommand{\Ind}{\ensuremath{\mathbb{1}}}
\newcommand{\im}{\ensuremath{\mathbf{i}}}

\newcommand{\argmin}{\operatorname*{argmin}}
\newcommand{\argmax}{\operatorname*{argmax}}
% \renewcommand\qedsymbol{\rule{1ex}{1ex}}

% course specifics
\renewcommand{\P}{\operatorname*{\ensuremath{\mathbf{P}}}} % probability measure
\newcommand{\Ev}{\operatorname*{\ensuremath{\mathbf{E}}}} %expected value
\newcommand{\Fa}{\ensuremath{\mathcal{F}}} %sigma-algebra
\newcommand{\cH}{\ensuremath{\mathcal{H}}}
\newcommand{\cD}{\ensuremath{\mathcal{D}}}
\newcommand{\cX}{\ensuremath{\mathcal{X}}}
\newcommand{\cY}{\ensuremath{\mathcal{Y}}}
\newcommand{\sgn}{\mathrm{sgn}}

\newcommand{\bw}{\ensuremath{\mathbf{w}}} % weight vector
\newcommand{\bl}{\ensuremath{\mathbf{l}}} % loss vector

% Solutions use a modified proof environment
\newenvironment{solution}
               {\let\oldqedsymbol=\qedsymbol
                \renewcommand{\qedsymbol}{$\blacktriangleleft$}
                \begin{proof}[Solution]}
               {\end{proof}
                \renewcommand{\qedsymbol}{\oldqedsymbol}}
                
\newtcolorbox{definitionbox}[1]{%
    tikznode boxed title,
    enhanced,
    arc=0mm,
    interior style={white},
    attach boxed title to top center= {yshift=-\tcboxedtitleheight/2},
    fonttitle=\bfseries,
    colbacktitle=white,coltitle=black,
    boxed title style={size=normal,colframe=white,boxrule=0pt},
    title={#1}}


\newcommand{\TODO}{\textcolor{red}{\textbf{!!!!!! }}}

\begin{document}
\begin{center}
    {\huge\bf Machine learning theory}\\
    {\large\sc Homework set 9 }\\ \vspace{1em}
    {Aras} \textsc{ {Selvi}} \\
    {Daniel} \textsc{ {Figueroa}}\\
    {Fredrik} \textsc{ {Gustafsson}}\\
    {Lasse} \textsc{ {Vuursteen}}\\
    {Martijn} \textsc{ {Bouman}}\\
    {Ruud} \textsc{ {Nimour}}\\
    {Tamar} \textsc{ {Huygen}}\\
    {Twan} \textsc{ {Koperberg}}\\
    {Vincent} \textsc{ {Hennink}}\\ 
    \bigskip
    \today \bigskip
    \hrule
    \bigskip
\end{center}

\section{Background}
\begin{definitionbox}{Definition 1 (Adversarial Bandit Setting).}
  \textbf{Protocol:}\\
  Adversary chooses $l_{t,k} \in [0,1]$ for all $t \leq T$, $k \leq K$.\\
  For $t = 1,2, \ldots$
\begin{itemize}
    \item Learner picks arm $I_t$ (tipically by sampling $I_t \sim \mathbf{p}_t$)
    \item Learner observes and incurs \textit{loss} $l_{t, I_{t}}$
\end{itemize}
\textbf{Objective:} Expected Regret w.r.t. best expert after $T$ rounds:
\begin{equation*}
    \bar{R}_T = \Ev_{I_1 , \ldots , I_T} \cbrac*{\sum^T_{t=1} l_t^k} - \min_k \sum_{t=1}^T l_t^k .
\end{equation*}
\end{definitionbox}
\begin{definitionbox}{Definition 2 (Stochastic Bandit setting).}
  Environment: distributions ($\nu_1 , \ldots \nu_K$) of arm rewards.\\
\textbf{Protocol:} For $t = 1,2, \ldots$
\begin{itemize}
    \item Learner picks arm $I_t$
    \item Learner observes and receives \textit{reward} $X_t \sim \nu_{I_t}$
\end{itemize}
\textbf{Objective:} Pseudo-Regret w.r.t. best expert after $T$ rounds:
\begin{equation*}
    \bar{R}_T = T \max_{k} \Ev_{X \sim \nu_k} \left[ X \right] - \Ev_{I_1 , \ldots , I_T} \cbrac*{ \sum^T_{t=1} X_t} .
\end{equation*}
\end{definitionbox}
\subsection{Confidence Intervals}
You can use the following bound, known as Chernoff's bound for Gausssians.
For $X_1,\ldots ,X_n$ i.i.d. unit-variance Gaussian random variables, the empirical estimate $\hat{\mu}_n = \frac{1}{n} \sum _{i=1}^n X_i$ satisfies
\begin{align*}
    & \P(\hat{\mu}_n - \mu \geq \varepsilon) \leq e^{-\tfrac{n}{2}\varepsilon^2} && \mbox{and} && \P(\hat{\mu}_n - \mu \leq -\varepsilon) \leq e^{-\tfrac{n}{2}\varepsilon^2}.
\end{align*}
This is Equation (2.3) in the Bubeck \& Cesa-Bianchi monograph, with $\psi (\lambda) = \lambda^2/2$ and $\psi^* (\varepsilon) = \varepsilon^2/2$.
\section{Exercises}
\subsection*{1. Legendre-Fenchel for Gaussian Chernoff}
Let $\psi(\lambda) = \frac{\lambda^2}{2}$. The legendre-Fenchel transform of $\psi$ is given by
\begin{equation*}
    \psi^*(\varepsilon) = \sup_{\lambda \in \R}\lambda \varepsilon - \psi(\lambda).
\end{equation*}
Show that
\subsubsection*{a}
$\psi^*(\varepsilon) = \frac{\varepsilon^2}{2}$.
\subsubsection*{b}
$(\psi^*)^{-1}(z) = \pm \sqrt{2z}$.
\subsection*{Bigger Picture} the purpose of this exercise is to make the Bubeck/Cesa-Bianchi reading material on UCB more readable

\subsection*{2. The Blooper Reel}
\begin{itemize}
    \item \textbf{Deterministic fails for Adversarial Bandits} Show that any deterministic algorithm (UCB included) has linear regret in the adversarial bandit setting. Hint: you can use the argument on the top of page 23.
\end{itemize}  
\begin{solution}
We assume that the adversary chooses the losses $\ell_{t}^{i}$ at time $t$ dependent on the distribution $p_t \in \triangle_K$ chosen at time $t$ by the learner. The learner chooses an action deterministically at each time step, 
i.e. $p_t \in \cbrac{\delta_i \c i \in [K]}$ for all $t$, 
where $\delta_i$ denotes the Dirac measure on $[K]$ with $\delta_i(\cbrac{i})=1$.
Now set
\begin{equation*}
 \ell_{t}^{i}=
 \begin{cases}
  1 & \text{ if }p_t=\delta_i \\
  0 & \text{ otherwise}.
 \end{cases}
\end{equation*}
Then for each deterministic strategy we have for the pseudo-regret at time $T$ that 
\begin{align*}
 \bar{R}_T &= \Ev\paren*{\sum_{t \in [T]} \ell_{t}^{I_t}}-\min_{k \in [K]}\sum_{t \in [T]}\ell_{t}^{k} \\
 &= T-\min_{k \in [K]}\sum_{t \in [T]}\ell_{t}^{k} \\
 &\geq T-\frac{T}{k} \\
 &= \frac{k-1}{k}T.
\end{align*}
So, the learner will incur at least a pseudo-regret that is linear in $T$.
\end{solution}

\begin{itemize}
    \item \textbf{ERM fails for Stochastic Bandits} \\
    Consider a $K$-armed stochastic bandit model with unit-variance Gaussian rewards with means $\mu_1,\ldots ,\mu_K$. In round $t$ the learner chooses arm $I_t \in [K]$ and receives reward $X_t \sim \mathcal{N}(\mu_{I_t}, 1)$, where $\mu_i$ is the (unknown) reward of arm $i$. Now let's fix te following algorithm, which is inspired by Empirical Risk Minimisation:
    \subsubsection*{a} First, pull every arm once (that is, $I_t = t$ for $t \leq K$).
    \subsubsection*{b} Then after each number $t \geq K$ of rounds, form the empirical estimates
    \begin{equation*}
        \hat{\mu}_i(t) = \frac{\sum_{s=1}^t \Ind_{\cbrac{I_s = i}} X_s}{\sum_{s=1}^t \Ind_{\cbrac{I_s =i}}}
    \end{equation*}
    and play $I_{t+1} = \argmax_i \hat{\mu}_i(t)$.\\
    For $K = 2$ show that this algorithm has pseudo-regret
    \begin{equation*}
        \bar{R}_T = T\mu^* - \Ev\paren*{\sum_{t \in [T]} \mu_{I_t}}
    \end{equation*}
    that is \textit{linear} in $T$.\\
    Hint: you can use the following outline. Assume $\mu_1 > \mu_2$. Pick some threshold $\varepsilon >0$ (which you will optimize in an later step).
    \begin{itemize}
        \item[--] Argue that with constant probability (independent of $n$) the reward drawn from the best arm in the first phase is below $\mu_2 - \varepsilon$.
        \item[--] Bound the probability that for a single time step $t$ we have $\hat{\mu}_2(t) < \mu_2 - \varepsilon$ using Chernoff's bound.
        \item[--] Use the union bound to bound the probability that $\exists t \geq 2 : \hat{\mu}_2(t) < \mu_2 - \varepsilon$
        \item[--] Now pick $\varepsilon$ large enough so that the previous probability bound is non-trivial (i.e. is $<1$).
    \end{itemize}
    Conclude that with some small probability the sample from the best arm is very low, and the samples from the second-best arm are all typical, so the algorithm keeps pulling arm 2 only. Deduce that the pseudo-regret is hence linear in $n$.
\end{itemize}
\begin{solution}
Without loss of generality, we can assume that $\mu_1 > \mu_2$, so that $\mu^*=\mu_1$.
For each time step $t$ let $Y_t^1 \sim N(\mu_1, 1)$ denote the reward of arm $1$ and $Y_t^2 \sim N(\mu_2, 1)$ the reward of arm $2$. Note that all $Y_t^1$ and $Y_t^2$ are independent. We now take $X_t=Y_t^1 \Ind_{\cbrac{I_t=1}}+Y_t^2 \Ind_{\cbrac{I_t=2}}$.
So, we have that $X_1 = Y_1$ and $X_2=Y_2$. Take $p=\P(X_1 < \mu_2)$ and note that $p > 0$.
For each $T \geq 2$ let $B_T$ denote the event that the reward of the first arm at time step $t=1$ is less than $\mu_2$ and 
that for each $2 \geq t \leq T$ the reward of arm $2$ at time $t$ is at least $\mu_2$. That is
\begin{equation*}
 B_T= \cbrac{Y^1_1 < \mu_2} \cap \bigcap_{t=2,\ldots,T}\cbrac{Y_t^2 > \mu_2}.
\end{equation*}
By independence we have that $\P(B_T)=p\paren*{\tfrac{1}{2}}^{T-1}$.
Note that $\bigcap_{t=2,\ldots,T}\cbrac{I_t=2} \se B_T$. 
So if event $B_T$ happens, then we only pick the second arm between time steps $t=2$ and $t=T$.
It follows that 
\begin{align*}
 \bar{R}_T &= T \mu_1 - \Ev\paren*{\sum_{t \in [T]} \mu_{I_t}} \\
 &= T \mu_1 - \Ev\paren*{\Ind_{B_T}\sum_{t \in [T]} \mu_{I_t}}-\Ev\paren*{\Ind_{B_T^C}\sum_{t \in [T]} \mu_{I_t}} \\
 &\geq T \mu_1 - \Ev\paren*{\Ind_{B_T}\sum_{t \in [T]} \mu_{I_t}}-T\mu_1(1-\P(B_T)) \\
 &= \P(B_T)T \mu_1 - \Ev\paren*{\Ind_{B_T}\sum_{t \in [T]} \mu_{I_t}} \\
 &= \P(B_T)T \mu_1 - \P(B_T)(\mu_1+(T-1)\mu_{2})  \\
 &= \P(B_T) (\mu_1-\mu_2)T - \P(B_T)(\mu_1-\mu_2).
\end{align*}
So, the regret increases linearly in $T$.
\end{solution}



\subsection*{3. Adversarial Semi-bandit}
We consider an adversarial bandit model with $K^2$ arms indexed by $i \in [K]$ and $j \in [K]$. For each arm $(i,j)$, the loss at time $t$ is $a_t^i + b_t^j$, where $a_t^i \in [0,1]$ and $b_t^j \in [0,1]$ are chosen by the adversary before the start of the interaction. Then each round the learner picks an arm $(I_t, J_t) \in [K]^2$ and observes $a_t^{I_t}$ and $b_t^{J_t}$ \textit{separately} (and incurs their sum as loss).
\subsubsection*{a} Consider running a single instance of EXP$3$ on all $K^2$ arms (with loss range $[0,2]$). Show that the expected pseudo-regret compared to the best arm $(i^*, j^*)$ is bounded by
\begin{equation*}
    \bar{R}_n \leq 2 \sqrt{2nK^2\ln\paren*{K^2}}.
\end{equation*}
\begin{solution}
  Note that the regret as defined in Definition 1, is defined with a loss in range $[0,1]$.
  Now we have that there are $K^2$ arms instead of $K$ arms and we can just fill in $K^2$ for the number of arms and the loss is in range $[0,2]$ instead of $[0,1]$.
  This means that if we define the loss as $l'_t^{(i,j)} =  \frac{1}{2}a_t^i + \frac{1}{2} b_t^j $, the range of $l'_t^{(i,j)}$ is $[0,1]$. Now we can see that for the regret it holds that
  \begin{align*}
    \bar{R}'_n  &= \Ev_{(I_1,J_1), \ldots , (I_n,J_n)}\cbrac*{\sum_{t=1}^n l'_{t, (I_t, J_t)}} - \min_{(i,j)} \sum_{t=1}^n l'_t^{(i,j)}\\
                &= \Ev_{(I_1,J_1), \ldots , (I_n,J_n)}\cbrac*{\sum_{t=1}^n \frac{1}{2}l_{t, (I_t, J_t)}} - \min_{(i,j)} \sum_{t=1}^n \frac{1}{2}l_t^{(i,j)}\\
                &= \frac{1}{2} \paren*{\Ev_{(I_1,J_1), \ldots , (I_n,J_n)}\cbrac*{\sum_{t=1}^n l_{t, (I_t, J_t)}} - \min_{(i,j)} \sum_{t=1}^n l_t^{(i,j)}}\\
                &= \frac{1}{2} \bar{R}_n
  \end{align*}

  Recall that in the adversarial bandit setting, with $K$ arms and $n$ time steps, the pseudo-regret of the EXP$3$ strategy is bounded by $\sqrt{2nK \ln K}$.
  Now we have $K^2$ arms instead of $K$ arms, combined with the previous result this gives:
  \begin{align*}
    \frac{1}{2} \bar{R}_n = \bar{R}'_n & \leq \sqrt{2nK^2 \ln K^2}\\
    \implies \bar{R}_n & \leq 2 \sqrt{2nK^2\ln\paren*{K^2}}.
  \end{align*}
\end{solution}    
\subsubsection*{b} Now we will use the $a_t^i$ and $b_t^j$ observations separately. Consider running two $K$-arm instances of EXP$3$, one with $i \mapsto a_t^i$ as the loss and one with $j \mapsto b_t^j$ as the loss. Have the first algorithm control $I_t$ and the second $J_t$. Show that the overall expected pseudo-regret is bounded by 
\begin{equation*}
    \bar{R}_n \leq 2 \sqrt{2nK\ln K}.
\end{equation*}
\begin{solution}
  Let $\bar{R}_{a,n}$ be the pseudo-regret of the $K$-arm instance with loss $i \mapsto a_t^i$ and let $\bar{R}_{b,n}$ be the pseudo-regret of the $K$-arm instance with loss $j \mapsto b_t^j$.
  The overall pseudo-regret is equal to the sum of both pseudo-regrets: $\bar{R}_n = \bar{R}_{a,n} + \bar{R}_{b,n}$. We know that $\bar{R}_{a,n}$ and $\bar{R}_{b,n}$ are both bounded by $\sqrt{2nK \ln K}$. So
  \begin{equation*}
    \bar{R}_n = \bar{R}_{a,n} + \bar{R}_{b,n} \leq \sqrt{2nK \ln K} + \sqrt{2nK \ln K} = 2\sqrt{2nK \ln K}.
\end{equation*}
\end{solution}
\subsubsection*{Bigger Picture} We see that we win a factor $\sqrt{K}$ by taking the structure of observations into account. There are many interesting intermediate observation models where we see interpolation between the full invormation regret $\sqrt{T \ln K}$ and the full bandit regret $\sqrt{KT}$.
\end{document}
%%% End:



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
