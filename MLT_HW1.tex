\documentclass[10pt, a4paper, twoside]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
%\usepackage[utf8x]{inputenc}
\usepackage{amsmath, amssymb, amsthm, amsfonts, mathrsfs, amsfonts}
\usepackage{mathtools} 

\usepackage{enumerate}

\usepackage[noabbrev]{cleveref}

%\usepackage{subfig}
\usepackage{pgf,tikz}
%\usetikzlibrary{arrows}

%\usepackage{natbib}
\usepackage[osf]{mathpazo}
\usepackage{euler}


% bold
\newcommand{\F}{\ensuremath{\mathbb{F}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
% 
% calligraphic
\newcommand{\A}{\ensuremath{\mathcal{A}}}
\newcommand{\E}{\ensuremath{\mathcal{E}}}
\newcommand{\I}{\ensuremath{\mathcal{I}}}

% Delimiters (requires mathtools package)
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\brac[]
\DeclarePairedDelimiter\cbrac\{\}
\DeclarePairedDelimiter\paren()
\DeclarePairedDelimiter{\ip}\langle\rangle
\DeclarePairedDelimiter{\nrm}\lVert\rVert

% Power set
\newcommand{\Ps}{\ensuremath{\mathcal{P}}}

 
% other things ...
\renewcommand{\c}{\ensuremath{\colon}}
\newcommand{\se}{\ensuremath{\subseteq}}
\renewcommand{\d}{\ensuremath{\ d}}
\newcommand{\Ind}{\ensuremath{\mathbb{1}}}
\newcommand{\im}{\ensuremath{\mathbf{i}}}

\newcommand{\argmin}{\operatorname*{argmin}}
\newcommand{\argmax}{\operatorname*{argmax}}
% \renewcommand\qedsymbol{\rule{1ex}{1ex}}

% course specifics
\newcommand{\Neighbour}{\ensuremath{\mathcal{N}}}
\renewcommand{\P}{\operatorname*{\ensuremath{\mathbf{P}}}}
\newcommand{\Ev}{\operatorname*{\ensuremath{\mathbf{E}}}}
\newcommand{\StochDom}{\ensuremath{\stackrel{\mathcal{D}}{\preceq}}}
\newcommand{\sgn}{\mathrm{sgn}}

% Solutions use a modified proof environment
\newenvironment{solution}
               {\let\oldqedsymbol=\qedsymbol
                \renewcommand{\qedsymbol}{$\blacktriangleleft$}
                \begin{proof}[Solution]}
               {\end{proof}
                \renewcommand{\qedsymbol}{\oldqedsymbol}}


\newcommand{\TODO}{\textcolor{red}{\textbf{!!!!!! }}}

\newcommand{\firstName}  {Twan}
\newcommand{\lastName}   {Koperberg}
\newcommand{\studId}     {0713309 (Leiden)}
\renewcommand{\email}    {twankop@gmail.com}

\newcommand{\firstNameII}  {Tamar}
\newcommand{\lastNameII}   {Huygen}
\newcommand{\studIdII}     {10907483 (UvA)}
\newcommand{\studIdIII}    {2556474 (VU)}
\newcommand{\emailII}     {tamar@huygen.nl}

\begin{document}
\begin{center}
  {\huge\bf Machine learning theory}\\
  {\large\sc Homeworkset 2 }\\ \vspace{1em}
  \firstName \textsc{ \lastName}, {\sc s}\studId \\
  \email\text{}\\ \smallskip
  \firstNameII \textsc{ \lastNameII}, \studIdII, \studIdIII\\
  \emailII \\ \bigskip
  \today \\\bigskip
  \hrule
  \bigskip
\end{center}

\section*{Exercise 2.1}
We have shown that the predictor defined in Equation (2.3) leads to overfitting. 
While this predictor seems to be very unnatural, the goal of this exercise is to show 
that it can be described as a thresholded polynomial. 
That is, show that given a training set 
\begin{equation*}
S = \cbrac{(x_i, f(x_i))}_{i=1}^{m} \se (\R^d \times \cbrac{0,1})^m, 
\end{equation*}
there exists a polynomial $p_S$ such that $h_S(x) = 1$ if and only if $p_S(x)\geq 0$, 
where $h_S$ is as defined in Equation (2.3). It follows that learning the class of all 
thresholded polynomials using the ERM rule may lead to overfitting.
\begin{solution}
Let $h_S:\R^d\to\cbrac{0,1}$ be given by
\begin{equation*}
 h_S(x)=
 \begin{cases}
  f(x_i) & \text{ if there exists }i \in [m] \text{ such that } x=x_i, \\
  0 & \text{ otherwise.} \\  
 \end{cases}
\end{equation*}

Consider the polynomial $p_S:\R^d \to \R$ given by
\begin{equation*}
 p_S(x^1,x^2,\ldots,x^d)=-\prod_{i=1}^{m}\paren*{(1-f(x_i))+\sum_{j=1}^d(x^j-x_i^j)^2},
\end{equation*}
where $x^j$ denotes the $j$-th component of $x \in \R^d$.

Note that $(1-f(x_i))+\sum_{j=1}^d(x^j-x_i^j)^2 \geq 0$ for all $i \in [m]$.
This means that $p_{S}(x)\leq 0$ for all $x \in \R^d$. Also we have that $p_S(x)=0$ if and only if there exists an 
$x_i \in S|_x$ such that $x=x_i$ and $f(x_i)=1$. So, we have that $p_S(x)=0$ if and only if $h_S(x)=1$.
\end{solution}

\section*{Exercise 2.2}
Let $\mathcal{H}$ be a class of binary classifiers over a domain $\mathcal{X}$. 
Let $\mathcal{D}$ be an unknown distribution over $\mathcal{X}$, and let $f$ be the target hypothesis in $\mathcal{H}$. 
Fix some $h \in \mathcal{H}$. Show that the expected value of $L_S(h)$ over the choice of $S|_x$ equals $L_{(\mathcal{D},f)}(h)$,
namely,
\begin{equation*}
\Ev_{S|_{x}\sim \mathcal{D}^m} [L_S(h)]=L_{(\mathcal{D},f)}(h).
\end{equation*}
\begin{solution}
% Let $X_1, X_2, \ldots, X_m$ be $\mathcal{X}$-valued i.i.d. random variables with distribution $\mathcal{D}$ and 
% let $S=(X_1, X_2, \ldots, X_m)$. Let $Z$ be a random variable with uniform distribution on $[m]$.
% Then we also have that $X_{Z}$ is distributed according to $\mathcal{D}$.
% 
% Note that 
% \begin{align*}
% L_{S}(h)&=\frac{\abs{y \in S|_{x} \c h(y)\neq f(y)}}{\abs{S|_{x}}} \\
% &=\P_{X_{Z} \sim \mathcal{D}}(h(X_Z)\neq f(X_Z)|S|_{x}=S).
% \end{align*}
% 
% Assume that $\mathcal{X}$ is countable. 
% Then we have that
% \begin{align*}
% \Ev_{S|_{x}\sim \mathcal{D}^m} [L_S(h)]&=\sum_{\mathbf{x} \in \mathcal{X}^m}L_{\mathbf{x}}(h)\mathcal{D}^m(\mathbf{x}) \\
% \end{align*}
% 
% Then do something with the law of total probability.
% 
% \TODO
% 
% Random thoughts:
% Note that 
% \begin{align*}
%  L_{(\mathcal{D},f)}(h) &=\P\limits_{X \sim \mathcal{D}}(h(X) \neq f(X)) \\
%  &=\Ev\limits_{X \sim \mathcal{D}}(\Ind_{\cbrac{h(X) \neq f(X)}}).
% \end{align*}
% We also have that

We have that
\begin{align*}
 \Ev\limits_{S|_x \sim \mathcal{D}^m}(L_{S}(h)) 
 &=\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\frac{\abs{\cbrac{i\in [m] \c h(x_i) \neq y_i}}}{m}} \\
 &=\frac{1}{m}\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\abs{\cbrac{i\in [m] \c h(x_i) \neq y_i}}} \\
 &=\frac{1}{m}\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\sum_{i=1}^m \Ind_{\cbrac{h(x_i)\neq y_i}}} \\
 &=\frac{1}{m}\sum_{i=1}^m \Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\Ind_{\cbrac{h(x_i)\neq y_i}}} \\
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{S|_x \sim \mathcal{D}^m}\paren*{\cbrac{h(x_i)\neq y_i}} \\
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{x_i \sim \mathcal{D}}\paren*{\cbrac{h(x_i)\neq y_i}} \\
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{x_i \sim \mathcal{D}}\paren*{\cbrac{h(x_i)\neq f(x_i)}} \\
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{x \sim \mathcal{D}}\paren*{\cbrac{h(x)\neq f(x)}} \\
 &=\P\limits_{x \sim \mathcal{D}}\paren*{\cbrac{h(x)\neq f(x)}} \\
 &=L_{(\mathcal{D},f)}(h).
\end{align*}
\TODO Leg uit wat hier gebeurt.


\end{solution}


\section*{Exercise 2.3}
An axis aligned rectangle classifier in the plane is a classifier that assigns the value $1$ to a point 
if and only if it is inside a certain rectangle. 
Formally, given real numbers $a_1 \leq b_1$, $a_2 \leq b_2$, define the classifier $h_{(a_1, b_1, a_2, b_2)}:\R^2\to\cbrac{0,1}$ by
\begin{equation*}
h_{(a_1, b_1, a_2, b_2)}(x, y)=
\begin{cases}
 1 & \text{ if }x \in [a_1, b_1], \ y \in [a_2, b_2], \\
 0 & \text{ otherwise}.
\end{cases}
\end{equation*}

The class of all axis aligned rectangles in the plane is defined as
\begin{equation*}
\mathcal{H}^2_{rec}=\cbrac{h_{(a_1, b_1, a_2, b_2)} \c a_1 \leq b_1 \text{ and } a_2 \leq b_2}.
\end{equation*}
Note that this is an infinite size hypothesis class. Throughout this exercise we rely on the realizability assumption.

\subsection*{$1$}
\begin{solution}
We assume that algorithm $A$ returns $\Ind_{\varnothing}$ if no positive instances have been sampled.

By the realizability assumption, there exists a rectangle 
\begin{equation*}
R^*(a_1^*,b_1^*,a_2^*,b_2^*)=\cbrac{(x,y)\in \R^2 \c x \in [a_1^*,b_1^*] \text{ and }y \in [a_2^*, b_2^*]}
\end{equation*}
such that for all $(x, y) \in \R^2$ we have that
$h_{R^*}(x, y)=f(x, y)$. Let $S$ be a given training set and let $R(S)$ be the rectangle returned by algorithm $A$. 
Let $S|_{\mathbf{x}}=(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_m)$ denote the sequence of instances in the training set.

If there exists an instance $\mathbf{x}_i$ in $S|_{x}$ with $\mathbf{x}_i \notin R(S)$, 
then by construction we have that $f(\mathbf{x}_i)=0$.
Also, as $R(S)$ is the smallest rectangle containing all $\mathbf{x}_i$ with $f(\mathbf{x}_i)=1$,
we have that $R(S)\se R^*$. It follows that $h_{R(S)}(\mathbf{x}_i)=f(\mathbf{x}_i)$ for all $i \in [m]$.
Thus we have that $L_S(h_{R(S)})=0$, so that $h_{R(S)} \in \argmin\limits_{h \in \mathcal{H}^2_{rec}}L_S(h)$.
Therefore, $A$ is an ERM.
\end{solution}

\subsection*{$2$}
Show that if $A$ receives a training set of size $\geq \frac{4 \log (4/\delta)}{\epsilon}$ then, with probability of at least $1-\delta$ it returns a hypothesis with error  of at most $\epsilon$.\\
<<<<<<< HEAD
\textit{Hint:} Fix some distribution
\textit{Show that $R(S)\subseteq R^*$.}\\

=======
\textit{Hint:} Fix some distribution $\mathcal{D}$.
\textit{Show that $R(S)\subseteq R^*$.}
>>>>>>> d919c531bae589ef03f8810855a2750b9403a5dd
\begin{solution}
All positive examples lie within $R^*$. That is, for any point $\mathbf{x} = (x_1,x_2) \in \R^2$ such that $f(\mathbf{x})=1$, it holds that $a_1^*\leq x_1 \leq b_1^*$ and that $a_2^*\leq x_2 \leq b_2^*$. Note that if there exist no $(x_1,x_2) \in S|_x$ such that $f(x_1,x_2)=1$, then $A$ will return $\emptyset \subseteq R^*$. Let $R(S)(a_1',a_2',b_1',b_2')$ be the rectangle that $A$ returns and let $S|_x = {\mathbf{x}^1, \mathbf{x}^2,\ldots, \mathbf{x}^m}$. If there exist points $(x^i_1,x^i_2)\in S|_x$ such that $f(x^i_1,x^i_2)=1$, then the borders of $R(S)$, go through such points within $R^*$, that is $a_1' = \min_{i\in [m]} x^i_1$ and $b_1' = \max_{i\in [m]} x^i_1$, similarly $a_2' = \min_{i\in [m]} \mathbf{x}^i_2$ and $b_2' = \max_{i\in [m]} \mathbf{x}^i_2$. So, it holds that $a_1^*\leq a_1'\leq b_1' \leq b_1^*$ and $a_2^* \leq a_2' \leq b_2' \leq b_2^*$. Now it holds that if a point $\mathbf{x} = (x_1, x_2)\in \R^2$ is an element of $R(S)$, that $a_1^*\leq a_1'\leq x_1 \leq b_1' \leq b_1^*$ and $a_2^* \leq a_2'\leq x_2 \leq b_2' \leq b_2^*$, so then it holds that $(x_1,x_2) \in R^*)$, so $R(S) \subseteq R^*$.\\
\end{solution}
\textit{Show that if $S$ contains (positive) examples in all of the rectangles, $R_1, R_2, R_3, R_4$, then the hypothesis, returned by $A$ has error of at most $\epsilon$.}
\subsection*{$3$}
\TODO



\section*{Exercise 4}
Let $a, b \in \R^n$. 
Show that the set of points that are closer to point $a$ than to point $b$ in Euclidean norm, 
i.e. the set $\cbrac{x \in \R^n \c \nrm{x-a}_2 \leq \nrm{x-b}_2}$, is a half space. 
Rewrite the half space in the form presented in the lecture: $w^T x \leq c$.

\begin{solution}
 Take $w=a-b$ and $c=\tfrac{1}{2}(\nrm{a}_2-\nrm{b}_2)$. Let $x \in \R^n$ be given.
 Then we have that 
 \begin{align*}
 \nrm{a-x}_2^2 -\nrm{b-x}_2^2 &= \ip{a-x, a-x}-\ip{b-x, b-x} \\
 &= \ip{a,a}-2\ip{a,x}-\ip{b,b} + 2\ip{b,x} \\
 &= \ip{a,a}-\ip{b,b} + 2\ip{b-a,x} \\
 &= \nrm{a}_2^2-\nrm{b}_2^2 + 2\ip{b-a,x} \\
 &= 2c - 2\ip{w,x}.
 \end{align*}
 It follows that $2c - 2\ip{w,x} \geq 0$ if and only if $x$ is closer to $a$ than $b$.
 Therefore, we have that 
 \begin{equation*}
  \cbrac{x \in \R^n \c \nrm{x-a}_2 \leq \nrm{x-b}_2}=\cbrac{x \in \R^n \c \ip{a-b,x} \leq \tfrac{1}{2}(\nrm{a}_2-\nrm{b}_2)}.
 \end{equation*}

 
\end{solution}

 



\end{document}


