\documentclass[10pt, a4paper, twoside]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
%\usepackage[utf8x]{inputenc}
\usepackage{amsmath, amssymb, amsthm, amsfonts, mathrsfs, amsfonts}
\usepackage{mathtools} 

\usepackage{enumerate}

\usepackage[noabbrev]{cleveref}

%\usepackage{subfig}
\usepackage{pgf,tikz}
%\usetikzlibrary{arrows}

%\usepackage{natbib}
\usepackage[osf]{mathpazo}
\usepackage{euler}


% bold
\newcommand{\F}{\ensuremath{\mathbb{F}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
% 
% calligraphic
\newcommand{\A}{\ensuremath{\mathcal{A}}}
\newcommand{\E}{\ensuremath{\mathcal{E}}}
\newcommand{\I}{\ensuremath{\mathcal{I}}}

% Delimiters (requires mathtools package)
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\brac[]
\DeclarePairedDelimiter\cbrac\{\}
\DeclarePairedDelimiter\paren()
\DeclarePairedDelimiter{\ip}\langle\rangle
\DeclarePairedDelimiter{\nrm}\lVert\rVert

% Power set
\newcommand{\Ps}{\ensuremath{\mathcal{P}}}

 
% other things ...
\renewcommand{\c}{\ensuremath{\colon}}
\newcommand{\se}{\ensuremath{\subseteq}}
\renewcommand{\d}{\ensuremath{\ d}}
\newcommand{\Ind}{\ensuremath{\mathbb{1}}}
\newcommand{\im}{\ensuremath{\mathbf{i}}}

\newcommand{\argmin}{\operatorname*{argmin}}
\newcommand{\argmax}{\operatorname*{argmax}}
% \renewcommand\qedsymbol{\rule{1ex}{1ex}}

% course specifics
\newcommand{\Neighbour}{\ensuremath{\mathcal{N}}}
\renewcommand{\P}{\operatorname*{\ensuremath{\mathbf{P}}}}
\newcommand{\Ev}{\operatorname*{\ensuremath{\mathbf{E}}}}
\newcommand{\StochDom}{\ensuremath{\stackrel{\mathcal{D}}{\preceq}}}
\newcommand{\sgn}{\mathrm{sgn}}

% Solutions use a modified proof environment
\newenvironment{solution}
               {\let\oldqedsymbol=\qedsymbol
                \renewcommand{\qedsymbol}{$\blacktriangleleft$}
                \begin{proof}[Solution]}
               {\end{proof}
                \renewcommand{\qedsymbol}{\oldqedsymbol}}


\newcommand{\TODO}{\textcolor{red}{\textbf{!!!!!! }}}

\newcommand{\firstName}  {Twan}
\newcommand{\lastName}   {Koperberg}
\newcommand{\studId}     {0713309 (Leiden)}
\renewcommand{\email}    {twankop@gmail.com}

\newcommand{\firstNameII}  {Tamar}
\newcommand{\lastNameII}   {Huygen}
\newcommand{\studIdII}     {10907483 (UvA)}
\newcommand{\studIdIII}    {2556474 (VU)}
\newcommand{\emailII}     {tamar@huygen.nl}

\begin{document}
\begin{center}
  {\huge\bf Machine learning theory}\\
  {\large\sc Homeworkset 1 }\\ \vspace{1em}
  \firstName \textsc{ \lastName}, {\sc s}\studId \\
  \email\text{}\\ \smallskip
  \firstNameII \textsc{ \lastNameII}, \studIdII, \studIdIII\\
  \emailII \\ \bigskip
  \today \\\bigskip
  \hrule
  \bigskip
\end{center}

\section*{Exercise 2.1}
We have shown that the predictor defined in Equation (2.3) leads to overfitting. 
While this predictor seems to be very unnatural, the goal of this exercise is to show 
that it can be described as a thresholded polynomial. 
That is, show that given a training set 
\begin{equation*}
S = \cbrac{(x_i, f(x_i))}_{i=1}^{m} \se (\R^d \times \cbrac{0,1})^m, 
\end{equation*}
there exists a polynomial $p_S$ such that $h_S(x) = 1$ if and only if $p_S(x)\geq 0$, 
where $h_S$ is as defined in Equation (2.3). It follows that learning the class of all 
thresholded polynomials using the ERM rule may lead to overfitting.
\begin{solution}
Let $h_S:\R^d\to\cbrac{0,1}$ be given by
\begin{equation*}
 h_S(x)=
 \begin{cases}
  f(x_i) & \text{ if there exists }i \in [m] \text{ such that } x=x_i, \\
  0 & \text{ otherwise.} \\  
 \end{cases}
\end{equation*}

Consider the polynomial $p_S:\R^d \to \R$ given by
\begin{equation*}
 p_S(x^1,x^2,\ldots,x^d)=-\prod_{i=1}^{m}\paren*{(1-f(x_i))+\sum_{j=1}^d(x^j-x_i^j)^2},
\end{equation*}
where $x^j$ denotes the $j$-th component of $x \in \R^d$.

Note that $(1-f(x_i))+\sum_{j=1}^d(x^j-x_i^j)^2 \geq 0$ for all $i \in [m]$.
This means that $p_{S}(x)\leq 0$ for all $x \in \R^d$. Also we have that $p_S(x)=0$ if and only if there exists an 
$x_i \in S|_x$ such that $x=x_i$ and $f(x_i)=1$. So, we have that $p_S(x)=0$ if and only if $h_S(x)=1$.
\end{solution}

\section*{Exercise 2.2}
Let $\mathcal{H}$ be a class of binary classifiers over a domain $\mathcal{X}$. 
Let $\mathcal{D}$ be an unknown distribution over $\mathcal{X}$, and let $f$ be the target hypothesis in $\mathcal{H}$. 
Fix some $h \in \mathcal{H}$. Show that the expected value of $L_S(h)$ over the choice of $S|_x$ equals $L_{(\mathcal{D},f)}(h)$,
namely,
\begin{equation*}
\Ev_{S|_{x}\sim \mathcal{D}^m} [L_S(h)]=L_{(\mathcal{D},f)}(h).
\end{equation*}
\begin{solution}
% Let $X_1, X_2, \ldots, X_m$ be $\mathcal{X}$-valued i.i.d. random variables with distribution $\mathcal{D}$ and 
% let $S=(X_1, X_2, \ldots, X_m)$. Let $Z$ be a random variable with uniform distribution on $[m]$.
% Then we also have that $X_{Z}$ is distributed according to $\mathcal{D}$.
% 
% Note that 
% \begin{align*}
% L_{S}(h)&=\frac{\abs{y \in S|_{x} \c h(y)\neq f(y)}}{\abs{S|_{x}}} \\
% &=\P_{X_{Z} \sim \mathcal{D}}(h(X_Z)\neq f(X_Z)|S|_{x}=S).
% \end{align*}
% 
% Assume that $\mathcal{X}$ is countable. 
% Then we have that
% \begin{align*}
% \Ev_{S|_{x}\sim \mathcal{D}^m} [L_S(h)]&=\sum_{\mathbf{x} \in \mathcal{X}^m}L_{\mathbf{x}}(h)\mathcal{D}^m(\mathbf{x}) \\
% \end{align*}
% 
% Then do something with the law of total probability.
% 
% \TODO
% 
% Random thoughts:
% Note that 
% \begin{align*}
%  L_{(\mathcal{D},f)}(h) &=\P\limits_{X \sim \mathcal{D}}(h(X) \neq f(X)) \\
%  &=\Ev\limits_{X \sim \mathcal{D}}(\Ind_{\cbrac{h(X) \neq f(X)}}).
% \end{align*}
% We also have that

Because of the linearity of $\Ev$, we have that
\begin{align*}
 \Ev\limits_{S|_x \sim \mathcal{D}^m}(L_{S}(h)) 
 &=\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\frac{\abs{\cbrac{i\in [m] \c h(x_i) \neq y_i}}}{m}} \\
  &=\frac{1}{m}\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\abs{\cbrac{i\in [m] \c h(x_i) \neq y_i}}} \\
\end{align*}
Let $\Ind_{\cbrac{h(x_i)\neq y_i }}$ be the indicator function, such that $\Ind(x_i) = 1$ if $h(x_i)\neq y_i$ and $\Ind(x_i) = 0$ otherwise. Then we have that
\begin{equation*}
  \frac{1}{m}\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\abs{\cbrac{i\in [m] \c h(x_i) \neq y_i}}} = \frac{1}{m}\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\sum_{i=1}^m \Ind_{\cbrac{h(x_i)\neq y_i}}} \\
\end{equation*}
And again because of the linearity of $\Ev$, we have that
\begin{equation*}
  \frac{1}{m}\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\sum_{i=1}^m \Ind_{\cbrac{h(x_i)\neq y_i}}}
  =\frac{1}{m}\sum_{i=1}^m \Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\Ind_{\cbrac{h(x_i)\neq y_i}}}
\end{equation*}
Because $\Ind_{\cbrac{h(x_i)\neq y_i}} = 1$ when  $h(x_i)\neq y_i$ and $0$ otherwise, we can see that 
\begin{align*}
 \frac{1}{m}\sum_{i=1}^m \Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\Ind_{\cbrac{h(x_i)\neq y_i}}}
  &=\frac{1}{m}\sum_{i=1}^m 1\P\limits_{S|_x \sim \mathcal{D}^m}\paren*{\cbrac{h(x_i)\neq y_i}} +0 \P\limits_{S|_x \sim \mathcal{D}^m}\paren*{\cbrac{h(x_i)= y_i}} \\
  &=\frac{1}{m}\sum_{i=1}^m \P\limits_{S|_x \sim \mathcal{D}^m}\paren*{\cbrac{h(x_i)\neq y_i}}
\end{align*}
And because $f(x_i) = y_i$, and because all the examples in the training set are i.i.d., we have that
\begin{align*}
  \frac{1}{m}\sum_{i=1}^m \P\limits_{S|_x \sim \mathcal{D}^m}\paren*{\cbrac{h(x_i)\neq y_i}}
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{x_i \sim \mathcal{D}}\paren*{\cbrac{h(x_i)\neq y_i}} \\
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{x_i \sim \mathcal{D}}\paren*{\cbrac{h(x_i)\neq f(x_i)}} \\
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{x \sim \mathcal{D}}\paren*{\cbrac{h(x)\neq f(x)}} \\
 &=\P\limits_{x \sim \mathcal{D}}\paren*{\cbrac{h(x)\neq f(x)}} \\
 &=L_{(\mathcal{D},f)}(h).
\end{align*}



\end{solution}


\section*{Exercise 2.3}
An axis aligned rectangle classifier in the plane is a classifier that assigns the value $1$ to a point 
if and only if it is inside a certain rectangle. 
Formally, given real numbers $a_1 \leq b_1$, $a_2 \leq b_2$, define the classifier $h_{(a_1, b_1, a_2, b_2)}:\R^2\to\cbrac{0,1}$ by
\begin{equation*}
h_{(a_1, b_1, a_2, b_2)}(x, y)=
\begin{cases}
 1 & \text{ if }x \in [a_1, b_1], \ y \in [a_2, b_2], \\
 0 & \text{ otherwise}.
\end{cases}
\end{equation*}

The class of all axis aligned rectangles in the plane is defined as
\begin{equation*}
\mathcal{H}^2_{rec}=\cbrac{h_{(a_1, b_1, a_2, b_2)} \c a_1 \leq b_1 \text{ and } a_2 \leq b_2}.
\end{equation*}
Note that this is an infinite size hypothesis class. Throughout this exercise we rely on the realizability assumption.

\subsection*{$1$}
\begin{solution}
We assume that algorithm $A$ returns $\Ind_{\varnothing}$ if no positive instances have been sampled.

By the realizability assumption, there exists a rectangle 
\begin{equation*}
R^*(a_1^*,b_1^*,a_2^*,b_2^*)=\cbrac{(x,y)\in \R^2 \c x \in [a_1^*,b_1^*] \text{ and }y \in [a_2^*, b_2^*]}
\end{equation*}
such that for all $(x, y) \in \R^2$ we have that
$h_{R^*}(x, y)=f(x, y)$. Let $S$ be a given training set and let $R(S)$ be the rectangle returned by algorithm $A$. 
Let $S|_{\mathbf{x}}=(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_m)$ denote the sequence of instances in the training set.

If there exists an instance $\mathbf{x}_i$ in $S|_{x}$ with $\mathbf{x}_i \notin R(S)$, 
then by construction we have that $f(\mathbf{x}_i)=0$.
Also, as $R(S)$ is the smallest rectangle containing all $\mathbf{x}_i$ with $f(\mathbf{x}_i)=1$,
we have that $R(S)\se R^*$. It follows that $h_{R(S)}(\mathbf{x}_i)=f(\mathbf{x}_i)$ for all $i \in [m]$.
Thus we have that $L_S(h_{R(S)})=0$, so that $h_{R(S)} \in \argmin\limits_{h \in \mathcal{H}^2_{rec}}L_S(h)$.
Therefore, $A$ is an ERM.
\end{solution}

\subsection*{$2$}
Show that if $A$ receives a training set of size $\geq \frac{4 \log (4/\delta)}{\epsilon}$ then, with probability of at least $1-\delta$ it returns a hypothesis with error  of at most $\epsilon$.\\
\textit{Hint:} Fix some distribution $\mathcal{D}$ over $\mathcal{X}$, let $R^* = R(a_1^*,b_1^*,a_2^*,b_2^*)$ be the rectangle that generates the labels and let $f$ be the corresponding hypothesis. Let $a_1 \geq a_1^*$ be a number such that the probability mass (with respect to $\mathcal{D}$) of the rectangle $R_1 = R(a_1^*,a_1,a_2^*,b_2^*)$ is exactly $\epsilon/4$. Similarly, let $b_1$, $a_2$, $b_2$ be numbers such that the probability masses of the rectangles $R_2 = R(b_1,b_1^*,a_2^*,b_2^*)$, $R_3 =  R(a_1^*,b_1^*,a_2^*,a_2$, $R_4 =  R(a_1^*,b_1^*,b_2,b_2^*$ are all exactly $\epsilon/4$. Let $R(S)$ be the rectangle returned by $A$.\\
Show that $R(S)\subseteq R^*$.
\begin{solution}
All positive examples lie within $R^*$. That is, for any point $\mathbf{x} = (x_1,x_2) \in \R^2$ such that $f(\mathbf{x})=1$, it holds that $a_1^*\leq x_1 \leq b_1^*$ and that $a_2^*\leq x_2 \leq b_2^*$. Note that if there exist no $(x_1,x_2) \in S|_x$ such that $f(x_1,x_2)=1$, then we will assume that $A$ will return $\emptyset \subseteq R^*$. Let $R(S) = R(a_1',b_1',a_2',b_2')$ be the rectangle that $A$ returns and let $S|_x = {\mathbf{x}^1, \mathbf{x}^2,\ldots, \mathbf{x}^m}$ with $\mathbf{x}^i = (x_1^i,x_2^i) \in \R^2$ and $i \in [m]$. Let $p \subseteq [m]$ be the indices of positive points (i.e. $f(\mathbf{x})=1$) in $S|_x$. If there exist points $(x^k_1,x^k_2)\in S|_x$ such that $f(x^k_1,x^k_2)=1$, with $k \in p$ then the borders of $R(S)$, go through such points within $R^*$, that is $a_1' = \min_{k\in p} x^p_1$ and $b_1' = \max_{k\in p} x^k_1$, similarly $a_2' = \min_{k\in p} \mathbf{x}^k_2$ and $b_2' = \max_{k\in p} \mathbf{x}^k_2$. So, it holds that $a_1^*\leq a_1'\leq b_1' \leq b_1^*$ and $a_2^* \leq a_2' \leq b_2' \leq b_2^*$. Now it holds that if a point $\mathbf{x} = (x_1, x_2)\in R(S)$, that $a_1^*\leq a_1'\leq x_1 \leq b_1' \leq b_1^*$ and $a_2^* \leq a_2'\leq x_2 \leq b_2' \leq b_2^*$, so then it holds that $(x_1,x_2) \in R^*)$, so $R(S) \subseteq R^*$.\\
\end{solution}
Show that if $S$ contains (positive) examples in all of the rectangles, $R_1, R_2, R_3, R_4$, then the hypothesis, returned by $A$ has error of at most $\epsilon$. \\
\begin{solution}
  Let $h'_{(a_1',b_1',a_2',b_2')}$ be the hypothesis that $A$ returns. $R(S)\subseteq R^*$, so for all freshly drawn examples $\mathbf{x} \in \R^2/R^*$,  it holds that $f(\mathbf{x})=h^*(\mathbf{x}) = h'(\mathbf{x}) = 0$. And because $R(S)\subseteq R^*$, for all freshly drawn examples $\mathbf{x} \in R(S)$, it holds that $f(\mathbf{x})=h^*(\mathbf{x}) = h'(\mathbf{x}) = 1$. This means that the error on all examples drawn outside of $R^*/R(S)$ equals $0$. For examples $\mathbf{x}$ drawn within $R^*/R(S)$ it holds that $f(\mathbf{x})=h^*(\mathbf{x}) = 1$ and $h'(\mathbf{x}) = 0$. This means it holds that $L_{(\mathcal{D},f)}(h') = \mathcal{D}(\{\mathbf{x}: f(mathbf{x}) \neq h'(\mathbf{x})\}) = \mathcal{D}(\{\mathbf{x}:\mathbf{x}\in R^*/R(S)\})$. \\
  At worst, the all the positive examples $\mathbf{x'} = (x_1',x_2') \in S|_x$ within $R_1$ lie on the line $\mathbf{x}=(x_1,x_2):x_1 = a_1$. That is, at worst, if $f(\mathbf{x'} = 1)$, it holds that $a_1\leq x_1' \leq b_1^*$. Because then the border of $R(S)$, will lie on the line $\mathbf{x} = (x_1,x_2) \in \R^2: x_1 = a_1$, that is $R(S) = R(a_1,b_1',a_2',b_2')$. Because then $R_{1}/\{\mathbf{x}:x_1=a_1\} \subseteq R^*/R(S)$ and all freshly drawn examples from $R_{1}/\{\mathbf{x}:x_1=a_1\}$ will be falsely classified. The chance that $\mathbf{x} = (x_1,x_2) \in S|_x$ is drawn from $R_1$ is $\frac{\epsilon}{4}$. Now we have that $\mathcal{D}(\{\mathbf{x}: \mathbf{x} \in R_{1}/\{\mathbf{x}:x_1=a_1\}\}) \leq \mathcal{D}(\{\mathbf{x}: \mathbf{x} \in R_{1}\}) = \frac{\epsilon}{4}$.\\
  Similar arguments hold for $R_2$, $R_3$ and $R_4$. We can see that at worst $R(S) = R(a_1,b_1,a_2,b_2)$ and so $(R^*/R(S)) \subseteq R_1 \cup R_2 \cup R_3 \cup R_4$. Now it holds that
\begin{align*}
  L_{\mathcal{D},f}(h') & = \mathcal{D}(\leq R^*/R(S)) \\
                      & \leq \mathcal{D}(R_1 \cup R_2 \cup R_3 \cup R_4) \\
                      & \leq \mathcal{D}(R_1) + \mathcal{D}(R_2) + \mathcal{D}(R_3) + \mathcal{D}(R_4)\\
 & = \frac{4\epsilon}{4} = \epsilon \\
\end{align*}  
\end{solution}
For each $i \in {1,\ldots,4}$, upper bound the probability that $S$ does not contain an example from $R_i$.\\
\begin{solution}
  We know that the chance an example $\mathbf{x} \in S|_x$ is drawn from $R_i$ with $i \in \{1,\ldots, 4\}$ equals $\P_{x~\mathcal{D}}(\mathbf{x}\in R_i) = \frac{\epsilon}{4}$, so the chance that an example is not drawn from $R_i$ is $\P_{x~\mathcal{D}}(\mathbf{x}\in \R^2/R_i) = 1-\frac{\epsilon}{4}$ This means that the chance that none of the samples in $S|_x$ is drawn from $R_i$ equals $\prod_{i = 1}^m(1-\frac{\epsilon}{4}) = (1-\frac{\epsilon}{4})^m$, with $m$ the size of the training set. Now we use that $(1-\epsilon)\leq e^{-\epsilon}$, to obtain that $(1-\frac{\epsilon}{4})^m \leq e^{- \frac{\epsilon}{4}m}$ and since $m \geq \frac{4\log (4/\delta)}{\epsilon}$, we have that $e^{- \frac{\epsilon}{4}m} \leq e^{-\frac{\epsilon}{4}\frac{4\log (4/\delta)}{\epsilon}}= \frac{\delta}{4}$
\end{solution}
Use the union bound to conclude this argument. \\
\begin{solution}
  It holds for the chance that none of the examples in $S|_x$ are drawn from $\cup_{i=1}^4 R_i$  that
\begin{align*}
  \mathcal{D}(\{\mathbf{x}:\mathbf{x}\in \R^2/(R_1 \cup R_2 \cup R_3 \cup R_4)\}) & \leq \sum_{i=1}^4 \mathcal{D}(\{\mathbf{x}:\mathbf{x}\in \R^2/R_i\}) \\
                                                                                 & = 4\frac{\delta}{4} = \delta \\
\end{align*}
So the chance that there is a positive sample in each of the rectangles is $1-\delta$. So now we can see that $L_{(\mathcal{D},f)}(h')\leq \epsilon$ with probability of at least $1 - \delta$.
\end{solution}
\subsection*{$3$}
Repeat the previous question for the class of axis aligned rectangles in $\R^d$. \\
Show that if $A$ receives a training set of size $\geq \frac{2d \log (2d/\delta)}{\epsilon}$ then, with probability of at least $1-\delta$ it returns a hypothesis with error  of at most $\epsilon$.\\
\textit{Hint:} Fix some distribution $\mathcal{D}$ over $\mathcal{X}$, let $R^* = R(a_1^*,b_1^*,\ldots,a_d^*,b_d^*)$ be the rectangle that generates the labels and let $f$ be the corresponding hypothesis. Let $a_1 \geq a_1^*$ be a number such that the probability mass (with respect to $\mathcal{D}$) of the rectangle $R_1 = R(a_1^*,a_1,a_2^*,b_2^*, \ldots)$ is exactly $\epsilon/4$. Similarly, let $b_1,a_2,b_2, \ldots,a_d,b_d$ be numbers such that the probability masses of the rectangles
\begin{align*}
  R_2 & = R(b_1,b_1^*,a_2^*,b_2^*,\ldots ,a_d^*,b_d^*), \\
  R_3 & =  R(a_1^*,b_1^*,a_2^*,a_2, \ldots , a_d^*, b_d^*,\\
  R_4 & =  R(a_1^*,b_1^*,b_2,b_2^*,\ldots , a_d^*,b_d^*), \\
  \ldots, \\
  R_{2d-1} & = R(a_1^*,b_1^*,b_2^*,b_2^*,\ldots , a_d^*,a_d),\\
  R_{2d} & = R(a_1^*,b_1^*,b_2^*,b_2^*,\ldots , b_d,b_d^*)\\
\end{align*}
  are all exactly $\frac{\epsilon}{2d}$. Let $R(S)$ be the rectangle returned by $A$.\\
Show that $R(S)\subseteq R^*$.
\begin{solution}
Let
\begin{equation*}
  h_{(a_1,b_1,\ldots,a_d,b_d)}(x_1,\ldots,x_d) = 
 \begin{cases}
  1 & if \forall i \in \{1,\ldots,d \} a_i \leq x_i \leq b_i \\
  0 & otherwise\\
\end{cases}
\end{equation*}
be the hypothesis class in $\R^d$.\\
All positive examples lie within $R^*$. That is, for any point $\mathbf{x} = (x_1,\ldots,x_d) \in \R^d$ such that $f(\mathbf{x})=1$, it holds that $\forall i \in \{1,\ldots,d\}$ it holds that $ a_i^*\leq x_i \leq b_i^*$. Let $S|_x = {\mathbf{x}^1, \mathbf{x}^2,\ldots, \mathbf{x}^m}$, with $\mathbf{x} = (x_1,\ldots,x_d) \in \R^d$. Note that if there exist no $\mathbf{x} \in S|_x$ such that $f(\mathbf{x})=1$, then we assume that $A$ will return $\emptyset \subseteq R^*$. Let $R(S)=R(a_1',b_1',\ldots,a_d',b_d')$ be the rectangle that $A$ returns. In other cases, let $p \subseteq [m]$ denote the indices of positive points in $S|_x$, that is at which $f(\mathbf{x}^k)=1$ with $k \in p$. If there exist such points $(x^k_1,\ldots,x^k_d)\in S|_x$  with $k \in p$ such that $f(x^k_1,\ldots,x^k_d)=1$, then the borders of $R(S)$, go through such points within $R^*$, that is $\forall i \in \{1,\ldots,d\}$ it holds that $ a_i' = \min_{k\in p} x^k_i$ and $\forall i \in \{1,\ldots,d\}$ it holds that $b_i' = \max_{k\in p} x^k_i$. So, we have that $\forall i \in \{1,\ldots,d\}$ it holds that $a_i^*\leq a_i'\leq b_i' \leq b_i^*$. Now we have that if a point $\mathbf{x} = (x_1,\ldots ,x_d)\in R(S)$, that $\forall i \in \{1,\ldots,d\}$ it holds that $a_i^*\leq a_i'\leq x_i \leq b_i' \leq b_i^*$, then $\mathbf{x} \in R^*)$, so $R(S) \subseteq R^*$.
\end{solution}
Show that if $S$ contains (positive) examples in all of the rectangles, $R_1,\ldots , R_d$, then the hypothesis, returned by $A$ has error of at most $\epsilon$. \\
\begin{solution}
    Let $h'_{(a_1',b_1',\ldots,a_d',b_d')}$ be the hypothesis that $A$ returns. $R(S)\subseteq R^*$, so for all freshly drawn examples $\mathbf{x} \in \R^d/R^*$,  it holds that $h^*(\mathbf{x})=f(\mathbf{x}) = h'(\mathbf{x}) = 0$. And because $R(S)\subseteq R^*$, for all freshly drawn examples $\mathbf{x} \in R(S)$, it holds that $h^*(\mathbf{x})=f(\mathbf{x}) = h'(\mathbf{x}) = 1$. This means that the error on all examples drawn outside of $R^*/R(S)$ equals $0$. For examples $\mathbf{x}$ drawn within $R^*/R(S)$ it holds that $h^*(\mathbf{x})=f(\mathbf{x}) = 1$ and $h'(\mathbf{x}) = 0$. This means it holds that $L_{(\mathcal{D},f)}(h') = \mathcal{D}(\{\mathbf{x}: f(mathbf{x}) \neq h'(\mathbf{x})\}) = \mathcal{D}(\{\mathbf{x}:\mathbf{x}\in R^*/R(S)\})$. \\
  At worst, the all the positive examples $\mathbf{x'} = (x_1',\ldots,x_d') \in S|_x$ within $R_1$ lie on the hyperplane $\mathbf{x}=(x_1,\ldots,x_d):x_1 = a_1$. That is, at worst, if $f(\mathbf{x'} = 1)$, it holds that $a_1\leq x_1' \leq b_1^*$. Because then the border of $R(S)$, will lie on the hyperplane $\mathbf{x} = (x_1,\ldots,x_d) \in \R^d: x_1 = a_1$, that is $R(S) = R(a_1,b_1',a_2',b_2',\ldots, a_d',b_d')$. Because then $R_{1}/\{\mathbf{x}:x_1=a_1\} \subseteq R^*/R(S)$ and all freshly drawn examples from $R_{1}/\{\mathbf{x}:x_1=a_1\}$ will be falsely classified. The chance that $\mathbf{x} \in S|_x$ is drawn from $R_1$ is $\frac{\epsilon}{2d}$. Now we have that $\mathcal{D}(\{\mathbf{x}: \mathbf{x} \in R_{1}/\{\mathbf{x}:x_1=a_1\}\}) \leq \mathcal{D}(\{\mathbf{x}: \mathbf{x} \in R_{1}\}) = \frac{\epsilon}{2d}$.\\
  Similar arguments hold for all $R_i$, with $i \in \{2, \ldots ,d\}$. We can see that at worst $R(S) = R(a_1,b_1,\ldots,a_d,b_d)$ and so $R^*/R(S) \subseteq \cup_{i-1}^{2d} R_i$. Now it holds that
\begin{align*}
  L_{\mathcal{D},f}(h') & = \mathcal{D}(\leq R^*/R(S)) \\
                      & \leq \mathcal{D}(\cup_{i=1}^{2d} R_i) \\
                      & \leq \sum_{i=1}^{2d} \mathcal{D}(R_i)\\
 & = \frac{2d\epsilon}{2d} = \epsilon \\
\end{align*}
\end{solution}
For each $i \in \{1,\ldots,2d\}$, upper bound the probability that $S$ does not contain an example from $R_i$.\\
\begin{solution}
  We know that the chance an example $\mathbf{x} \in S|_x$ is drawn from $R_i$ with $i \in \{1,\ldots, d\}$ equals $\P_{x~\mathcal{D}}(\mathbf{x}\in R_i) = \frac{\epsilon}{2d}$, so the chance that an example is not drawn from $R_i$ is $\P_{x~\mathcal{D}}(\mathbf{x}\in \R^d/R_i) = 1-\frac{\epsilon}{2d}$ This means that the chance that none of the samples in $S|_x$ is drawn from $R_i$ equals $\prod_{i = 1}^m(1-\frac{\epsilon}{2d}) = (1-\frac{\epsilon}{2d})^m$, with $m$ the size of the training set. Now we use that $(1-\epsilon)\leq e^{-\epsilon}$, to obtain that $(1-\frac{\epsilon}{2d})^m \leq e^{- \frac{\epsilon}{2d}m}$ and since $m \geq \frac{2d\log (2d/\delta)}{\epsilon}$, we have that $e^{- \frac{\epsilon}{2d}m} \leq e^{-\frac{\epsilon}{2d}\frac{2d\log (2d/\delta)}{\epsilon}}= \frac{\delta}{2d}$
\end{solution}
Use the union bound to conclude this argument. \\
\begin{solution}
  It holds for the chance that none of the examples in $S|_x$ are drawn from $\cup_{i=1}^{2d} R_i$  that
\begin{align*}
  \mathcal{D}(\{\mathbf{x}:\mathbf{x}\in \R^d/\cup_{i=1}^{2d} R_i\}) & \leq \sum_{i=1}^{2d} \mathcal{D}(\{\mathbf{x}:\mathbf{x}\in \R^d/R_i\}) \\
                                                                                 & = 2d\frac{\delta}{2d} = \delta \\
\end{align*}
So the chance that there is a positive sample in each of the rectangles is $1-\delta$. So now we can see that $L_{(\mathcal{D},f)}(h')\leq \epsilon$ with probability of at least $1 - \delta$.
\end{solution}

\section*{Exercise 4}
Let $a, b \in \R^n$. 
Show that the set of points that are closer to point $a$ than to point $b$ in Euclidean norm, 
i.e. the set $\cbrac{x \in \R^n \c \nrm{x-a}_2 \leq \nrm{x-b}_2}$, is a half space. 
Rewrite the half space in the form presented in the lecture: $w^T x \leq c$.

\begin{solution}
 Take $w=b-a$ and $c=\tfrac{1}{2}(\nrm{b}_2-\nrm{a}_2)$. Let $x \in \R^n$ be given.
 Then we have that 
 \begin{align*}
 \nrm{b-x}_2^2 -\nrm{a-x}_2^2 &= \ip{b-x, b-x}-\ip{a-x, a-x} \\
 &= \ip{b,b}-2\ip{b,x}-\ip{a,a} + 2\ip{a,x} \\
 &= \ip{b,b}-\ip{a,a} + 2\ip{a-b,x} \\
 &= \nrm{b}_2^2-\nrm{a}_2^2 + 2\ip{a-b,x} \\
 &= 2c - 2\ip{w,x}.
 \end{align*}
 It follows that $2c - 2\ip{w,x} \geq 0$ if and only if $x$ is closer to $a$ than $b$.
 Therefore, we have that 
 \begin{equation*}
  \cbrac{x \in \R^n \c \nrm{x-a}_2 \leq \nrm{x-b}_2}=\cbrac{x \in \R^n \c \ip{b-a,x} \leq \tfrac{1}{2}(\nrm{b}_2-\nrm{a}_2)}.
\end{equation*}

 
\end{solution}

 



\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
