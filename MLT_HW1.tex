\documentclass[10pt, a4paper, twoside]{amsart}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
%\usepackage[utf8x]{inputenc}
\usepackage{amsmath, amssymb, amsthm, amsfonts, mathrsfs, amsfonts}
\usepackage{mathtools} 

\usepackage{enumerate}

\usepackage[noabbrev]{cleveref}

%\usepackage{subfig}
\usepackage{pgf,tikz}
%\usetikzlibrary{arrows}

%\usepackage{natbib}
\usepackage[osf]{mathpazo}
\usepackage{euler}


% bold
\newcommand{\F}{\ensuremath{\mathbb{F}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}}
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}
\newcommand{\Q}{\ensuremath{\mathbb{Q}}}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\C}{\ensuremath{\mathbb{C}}}
% 
% calligraphic
\newcommand{\A}{\ensuremath{\mathcal{A}}}
\newcommand{\E}{\ensuremath{\mathcal{E}}}
\newcommand{\I}{\ensuremath{\mathcal{I}}}

% Delimiters (requires mathtools package)
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\brac[]
\DeclarePairedDelimiter\cbrac\{\}
\DeclarePairedDelimiter\paren()
\DeclarePairedDelimiter{\ip}\langle\rangle
\DeclarePairedDelimiter{\nrm}\lVert\rVert

% Power set
\newcommand{\Ps}{\ensuremath{\mathcal{P}}}

 
% other things ...
\renewcommand{\c}{\ensuremath{\colon}}
\newcommand{\se}{\ensuremath{\subseteq}}
\renewcommand{\d}{\ensuremath{\ d}}
\newcommand{\Ind}{\ensuremath{\mathbb{1}}}
\newcommand{\im}{\ensuremath{\mathbf{i}}}

\newcommand{\argmin}{\operatorname*{argmin}}
\newcommand{\argmax}{\operatorname*{argmax}}
% \renewcommand\qedsymbol{\rule{1ex}{1ex}}

% course specifics
\newcommand{\Neighbour}{\ensuremath{\mathcal{N}}}
\renewcommand{\P}{\operatorname*{\ensuremath{\mathbf{P}}}}
\newcommand{\Ev}{\operatorname*{\ensuremath{\mathbf{E}}}}
\newcommand{\StochDom}{\ensuremath{\stackrel{\mathcal{D}}{\preceq}}}
\newcommand{\sgn}{\mathrm{sgn}}

% Solutions use a modified proof environment
\newenvironment{solution}
               {\let\oldqedsymbol=\qedsymbol
                \renewcommand{\qedsymbol}{$\blacktriangleleft$}
                \begin{proof}[Solution]}
               {\end{proof}
                \renewcommand{\qedsymbol}{\oldqedsymbol}}


\newcommand{\TODO}{\textcolor{red}{\textbf{!!!!!! }}}

\newcommand{\firstName}  {Twan}
\newcommand{\lastName}   {Koperberg}
\newcommand{\studId}     {0713309 (Leiden)}
\renewcommand{\email}    {twankop@gmail.com}

\newcommand{\firstNameII}  {Tamar}
\newcommand{\lastNameII}   {Huygen}
\newcommand{\studIdII}     {10907483 (UvA)}
\newcommand{\studIdIII}    {2556474 (VU)}
\newcommand{\emailII}     {tamar@huygen.nl}

\begin{document}
\begin{center}
  {\huge\bf Machine learning theory}\\
  {\large\sc Homeworkset 2 }\\ \vspace{1em}
  \firstName \textsc{ \lastName}, {\sc s}\studId \\
  \email\text{}\\ \smallskip
  \firstNameII \textsc{ \lastNameII}, \studIdII, \studIdIII\\
  \emailII \\ \bigskip
  \today \\\bigskip
  \hrule
  \bigskip
\end{center}

\section*{Exercise 2.1}
We have shown that the predictor defined in Equation (2.3) leads to overfitting. 
While this predictor seems to be very unnatural, the goal of this exercise is to show 
that it can be described as a thresholded polynomial. 
That is, show that given a training set 
\begin{equation*}
S = \cbrac{(x_i, f(x_i))}_{i=1}^{m} \se (\R^d \times \cbrac{0,1})^m, 
\end{equation*}
there exists a polynomial $p_S$ such that $h_S(x) = 1$ if and only if $p_S(x)\geq 0$, 
where $h_S$ is as defined in Equation (2.3). It follows that learning the class of all 
thresholded polynomials using the ERM rule may lead to overfitting.
\begin{solution}
Let $h_S:\R^d\to\cbrac{0,1}$ be given by
\begin{equation*}
 h_S(x)=
 \begin{cases}
  f(x_i) & \text{ if there exists }i \in [m] \text{ such that } x=x_i, \\
  0 & \text{ otherwise.} \\  
 \end{cases}
\end{equation*}

Consider the polynomial $p_S:\R^d \to \R$ given by
\begin{equation*}
 p_S(x^1,x^2,\ldots,x^d)=-\prod_{i=1}^{m}\paren*{(1-f(x_i))+\sum_{j=1}^d(x^j-x_i^j)^2},
\end{equation*}
where $x^j$ denotes the $j$-th component of $x \in \R^d$.

Note that $(1-f(x_i))+\sum_{j=1}^d(x^j-x_i^j)^2 \geq 0$ for all $i \in [m]$.
This means that $p_{S}(x)\leq 0$ for all $x \in \R^d$. Also we have that $p_S(x)=0$ if and only if there exists an 
$x_i \in S|_x$ such that $x=x_i$ and $f(x_i)=1$. So, we have that $p_S(x)=0$ if and only if $h_S(x)=1$.
\end{solution}

\section*{Exercise 2.2}
Let $\mathcal{H}$ be a class of binary classifiers over a domain $\mathcal{X}$. 
Let $\mathcal{D}$ be an unknown distribution over $\mathcal{X}$, and let $f$ be the target hypothesis in $\mathcal{H}$. 
Fix some $h \in \mathcal{H}$. Show that the expected value of $L_S(h)$ over the choice of $S|_x$ equals $L_{(\mathcal{D},f)}(h)$,
namely,
\begin{equation*}
\Ev_{S|_{x}\sim \mathcal{D}^m} [L_S(h)]=L_{(\mathcal{D},f)}(h).
\end{equation*}
\begin{solution}
% Let $X_1, X_2, \ldots, X_m$ be $\mathcal{X}$-valued i.i.d. random variables with distribution $\mathcal{D}$ and 
% let $S=(X_1, X_2, \ldots, X_m)$. Let $Z$ be a random variable with uniform distribution on $[m]$.
% Then we also have that $X_{Z}$ is distributed according to $\mathcal{D}$.
% 
% Note that 
% \begin{align*}
% L_{S}(h)&=\frac{\abs{y \in S|_{x} \c h(y)\neq f(y)}}{\abs{S|_{x}}} \\
% &=\P_{X_{Z} \sim \mathcal{D}}(h(X_Z)\neq f(X_Z)|S|_{x}=S).
% \end{align*}
% 
% Assume that $\mathcal{X}$ is countable. 
% Then we have that
% \begin{align*}
% \Ev_{S|_{x}\sim \mathcal{D}^m} [L_S(h)]&=\sum_{\mathbf{x} \in \mathcal{X}^m}L_{\mathbf{x}}(h)\mathcal{D}^m(\mathbf{x}) \\
% \end{align*}
% 
% Then do something with the law of total probability.
% 
% \TODO
% 
% Random thoughts:
% Note that 
% \begin{align*}
%  L_{(\mathcal{D},f)}(h) &=\P\limits_{X \sim \mathcal{D}}(h(X) \neq f(X)) \\
%  &=\Ev\limits_{X \sim \mathcal{D}}(\Ind_{\cbrac{h(X) \neq f(X)}}).
% \end{align*}
% We also have that

We have that
\begin{align*}
 \Ev\limits_{S|_x \sim \mathcal{D}^m}(L_{S}(h)) 
 &=\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\frac{\abs{\cbrac{i\in [m] \c h(x_i) \neq y_i}}}{m}} \\
 &=\frac{1}{m}\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\abs{\cbrac{i\in [m] \c h(x_i) \neq y_i}}} \\
 &=\frac{1}{m}\Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\sum_{i=1}^m \Ind_{\cbrac{h(x_i)\neq y_i}}} \\
 &=\frac{1}{m}\sum_{i=1}^m \Ev\limits_{S|_x \sim \mathcal{D}^m}\paren*{\Ind_{\cbrac{h(x_i)\neq y_i}}} \\
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{S|_x \sim \mathcal{D}^m}\paren*{\cbrac{h(x_i)\neq y_i}} \\
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{x_i \sim \mathcal{D}}\paren*{\cbrac{h(x_i)\neq y_i}} \\
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{x_i \sim \mathcal{D}}\paren*{\cbrac{h(x_i)\neq f(x_i)}} \\
 &=\frac{1}{m}\sum_{i=1}^m \P\limits_{x \sim \mathcal{D}}\paren*{\cbrac{h(x)\neq f(x)}} \\
 &=\P\limits_{x \sim \mathcal{D}}\paren*{\cbrac{h(x)\neq f(x)}} \\
 &=L_{(\mathcal{D},f)}(h).
\end{align*}



\end{solution}


\section*{Exercise 2.3}
An axis aligned rectangle classifier in the plane is a classifier that assigns the value $1$ to a point 
if and only if it is inside a certain rectangle. 
Formally, given real numbers $a_1 \leq b_1$, $a_2 \leq b_2$, define the classifier $h_{(a_1, b_1, a_2, b_2)}:\R^2\to\cbrac{0,1}$ by
\begin{equation*}
h_{(a_1, b_1, a_2, b_2)}(x, y)=
\begin{cases}
 1 & \text{ if }x \in [a_1, b_1], \ y \in [a_2, b_2], \\
 0 & \text{ otherwise}.
\end{cases}
\end{equation*}

The class of all axis aligned rectangles in the plane is defined as
\begin{equation*}
\mathcal{H}^2_{rec}=\cbrac{h_{(a_1, b_1, a_2, b_2)} \c a_1 \leq b_1 \text{ and } a_2 \leq b_2}.
\end{equation*}
Note that this is an infinite size hypothesis class. Throughout this exercise we rely on the realizability assumption.

\subsection*{$1$}
\begin{solution}
We assume that algorithm $A$ returns $\Ind_{\varnothing}$ if no positive instances have been sampled.

By the realizability assumption, there exists a rectangle 
\begin{equation*}
R^*(a_1^*,b_1^*,a_2^*,b_2^*)=\cbrac{(x,y)\in \R^2 \c x \in [a_1^*,b_1^*] \text{ and }y \in [a_2^*, b_2^*]}
\end{equation*}
such that for all $(x, y) \in \R^2$ we have that
$h_{R^*}(x, y)=f(x, y)$. Let $S$ be a given training set and let $R(S)$ be the rectangle returned by algorithm $A$. 
Let $S|_{\mathbf{x}}=(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_m)$ denote the sequence of instances in the training set.

If there exists an instance $\mathbf{x}_i$ in $S|_{x}$ with $\mathbf{x}_i \notin R(S)$, 
then by construction we have that $f(\mathbf{x}_i)=0$.
Also, as $R(S)$ is the smallest rectangle containing all $\mathbf{x}_i$ with $f(\mathbf{x}_i)=1$,
we have that $R(S)\se R^*$. It follows that $h_{R(S)}(\mathbf{x}_i)=f(\mathbf{x}_i)$ for all $i \in [m]$.
Thus we have that $L_S(h_{R(S)})=0$, so that $h_{R(S)} \in \argmin\limits_{h \in \mathcal{H}^2_{rec}}L_S(h)$.
Therefore, $A$ is an ERM.
\end{solution}

\subsection*{$2$}
Show that if $A$ receives a training set of size $\geq \frac{4 \log (4/\delta)}{\epsilon}$ then, with probability of at least $1-\delta$ it returns a hypothesis with error  of at most $\epsilon$.\\
\textit{Hint:} Fix some distribution $\mathcal{D}$ over $\mathcal{X}$, let $R^* = R(a_1^*,b_1^*,a_2^*,b_2^*)$ be the rectangle that generates the labels and let $f$ be the corresponding hypothesis. Let $a_1 \geq a_1^*$ be a number such that the probability mass (with respect to $\mathcal{D}$) of the rectangle $R_1 = R(a_1^*,a_1,a_2^*,b_2^*)$ is exactly $\epsilon/4$. Similarly, let $b_1$, $a_2$, $b_2$ be numbers such that the probability masses of the rectangles $R_2 = R(b_1,b_1^*,a_2^*,b_2^*)$, $R_3 =  R(a_1^*,b_1^*,a_2^*,a_2$, $R_4 =  R(a_1^*,b_1^*,b_2,b_2^*$ are all exactly $\epsilon/4$. Let $R(S)$ be the rectangle returned by $A$.\\
\textit{Show that $R(S)\subseteq R^*$.}
\begin{solution}
All positive examples lie within $R^*$. That is, for any point $\mathbf{x} = (x_1,x_2) \in \R^2$ such that $f(\mathbf{x})=1$, it holds that $a_1^*\leq x_1 \leq b_1^*$ and that $a_2^*\leq x_2 \leq b_2^*$. Note that if there exist no $(x_1,x_2) \in S|_x$ such that $f(x_1,x_2)=1$, then $A$ will return $\emptyset \subseteq R^*$. Let $R(S)(a_1',a_2',b_1',b_2')$ be the rectangle that $A$ returns and let $S|_x = {\mathbf{x}^1, \mathbf{x}^2,\ldots, \mathbf{x}^m}$. If there exist points $(x^i_1,x^i_2)\in S|_x$ such that $f(x^i_1,x^i_2)=1$, then the borders of $R(S)$, go through such points within $R^*$, that is $a_1' = \min_{i\in [m]} x^i_1$ and $b_1' = \max_{i\in [m]} x^i_1$, similarly $a_2' = \min_{i\in [m]} \mathbf{x}^i_2$ and $b_2' = \max_{i\in [m]} \mathbf{x}^i_2$. So, it holds that $a_1^*\leq a_1'\leq b_1' \leq b_1^*$ and $a_2^* \leq a_2' \leq b_2' \leq b_2^*$. Now it holds that if a point $\mathbf{x} = (x_1, x_2)\in R(S)$, that $a_1^*\leq a_1'\leq x_1 \leq b_1' \leq b_1^*$ and $a_2^* \leq a_2'\leq x_2 \leq b_2' \leq b_2^*$, so then it holds that $(x_1,x_2) \in R^*)$, so $R(S) \subseteq R^*$.\\
\end{solution}
\textit{Show that if $S$ contains (positive) examples in all of the rectangles, $R_1, R_2, R_3, R_4$, then the hypothesis, returned by $A$ has error of at most $\epsilon$.} \\
\begin{solution}
  Let $h'_{(a_1',b_1',a_2',b_2')}$ be the hypothesis that $A$ returns. $R(S)\subseteq R^*$, so for all freshly drawn examples $\mathbf{x} \in \R^2/R^*$,  it holds that $f(\mathbf{x})=h^*(\mathbf{x}) = h'(\mathbf{x}) = 0$. And because $R(S)\subseteq R^*$, for all freshly drawn examples $\mathbf{x} \in R(S)$, it holds that $f(\mathbf{x})=h^*(\mathbf{x}) = h'(\mathbf{x}) = 1$. This means that the error on all examples drawn outside of $R^*/R(S)$ equals $0$. For examples $\mathbf{x}$ drawn within $R^*/R(S)$ it holds that $f(\mathbf{x})=h^*(\mathbf{x}) = 1$ and $h'(\mathbf{x}) = 0$. This means it holds that $L_{(\mathcal{D},f)}(h') = \mathcal{D}(\{\mathbf{x}: f(mathbf{x}) \neq h'(\mathbf{x})\}) = \mathcal{D}(\{\mathbf{x}:\mathbf{x}\in R^*/R(S)\})$. \\
  At worst, the all the positive examples $\mathbf{x'} = (x_1',x_2') \in S|_x$ within $R_1$ lie on the line $\mathbf{x}=(x_1,x_2):x_1 = a_1$. That is, at worst, if $f(\mathbf{x'} = 1)$, it holds that $a_1\leq x_1' \leq b_2^*$. Because then the border of $R(S)$, will lie on the line $\mathbf{x} = (x_1,x_2) \in \R^2: x_1 = a_1$, that is $R(S) = R(a_1,b_1',a_2',b_2')$. Because then $R_{1}/{\mathbf{x}:x_1=a_1} \subseteq R^*/R(S)$ and all freshly drawn examples from $R_{1}/{\mathbf{x}:x_1=a_1}$ will be falsely classified. The chance that $\mathbf{x} = (x_1,x_2) \in S|_x$ is drawn from $R_1$ is $\frac{\epsilon}{4}$. Now we have that $\mathcal{D}(\{\mathbf{x}: \mathbf{x} \in R_{1}/\{\mathbf{x}:x_1=a_1\}\}) \leq \mathcal{D}(\{\mathbf{x}: mathbf{x} \in R_{1}\}) = \frac{\epsilon}{4}$.\\
  Similar arguments hold for $R_2$, $R_3$ and $R_4$. We can see that at worst $R(S) = R(a_1,b_1,a_2,b_2)$ and so $R^*/R(S) \subseteq R_1 \cup R_2 \cup R_3 \cup R_4$. Now it holds that
\begin{align*}
  L_{\mathcal{D},f}(h') & = \mathcal{D}(\leq R^*/R(S)) \\
                      & \leq \mathcal{D}(R_1 \cup R_2 \cup R_3 \cup R_4) \\
                      & \leq \mathcal{D}(R_1) + \mathcal{D}(R_2) + \mathcal{D}(R_3) + \mathcal{D}(R_4)\\
 & = \frac{4\epsilon}{4} = \epsilon \\
\end{align*}  
\end{solution}
\textit{For each $i \in {1,\ldots,4}$, upper bound the probability that $S$ does not contain an example from $R_i$}\\
\begin{solution}
  We know that the chance an example $\mathbf{x} \in S|_x$ is drawn from $R_i$ with $i \in \{1,\ldots, 4\}$ equals $\P_{x~\mathcal{D}}(mathbf{x}\in R_i) = \frac{\epsilon}{4}$, so the chance that an example is not drawn from $R_i$ is $\P_{x~\mathcal{D}}(\mathbf{x}\in \R^2/R_i) = 1-\frac{\epsilon}{4}$ This means that the chance that none of the samples in $S|_x$ is drawn from $R_i$ equals $\prod_{i = 1}^m(1-\frac{\epsilon}{4}) = (1-\frac{\epsilon}{4})^m$, with $m$ the size of the training set. Now we use that $(1-\epsilon)\leq e^{-\epsilon}$, to obtain that $(1-\frac{\epsilon}{4})^m \leq e^{- \frac{\epsilon}{4}m}$ and since $m \geq \frac{4\log (4/\delta)}{\epsilon}$, we have that $e^{- \frac{\epsilon}{4}m} \leq e^{-\frac{\epsilon}{4}\frac{4\log (4/\delta)}{\epsilon}}= \frac{\delta}{4}$
\end{solution}
\textit{Use the union bound to conclude this argument.} \\
\begin{solution}
  It holds for the chance that none of the examples in $S|_x$ are drawn from $\Cup_{i=1}^4 R_i$  that
\begin{align*}
  \mathcal{D}(\{\mathbf{x}:\mathbf{x}\in \R^2/(R_1 \cup R_2 \cup R_3 \cup R_4)\}) & \leq \sum_{i=1}^4 \mathcal{D}(\{\mathbf{x}:\mathbf{x}\in \R^2/R_i\}) \\
                                                                                 & = 4\frac{\delta}{4} = \delta \\
\end{align*}
So the chance that there is a positive sample in each of the rectangles is $1-\delta$. So now we can see that $L_{(\mathcal{D},f)}(h')\leq \epsilon$ with probability of at least $1 - \delta$
\end{solution}
\subsection*{$3$}
Repeat the previous question for the class of axis aligned rectangles in $\R^d$

\TODO



\section*{Exercise 4}
Let $a, b \in \R^n$. 
Show that the set of points that are closer to point $a$ than to point $b$ in Euclidean norm, 
i.e. the set $\cbrac{x \in \R^n \c \nrm{x-a}_2 \leq \nrm{x-b}_2}$, is a half space. 
Rewrite the half space in the form presented in the lecture: $w^T x \leq c$.

\begin{solution}
 Take $w=a-b$ and $c=\tfrac{1}{2}(\nrm{a}_2-\nrm{b}_2)$. Let $x \in \R^n$ be given.
 Then we have that 
 \begin{align*}
 \nrm{a-x}_2^2 -\nrm{b-x}_2^2 &= \ip{a-x, a-x}-\ip{b-x, b-x} \\
 &= \ip{a,a}-2\ip{a,x}-\ip{b,b} + 2\ip{b,x} \\
 &= \ip{a,a}-\ip{b,b} + 2\ip{b-a,x} \\
 &= \nrm{a}_2^2-\nrm{b}_2^2 + 2\ip{b-a,x} \\
 &= 2c - 2\ip{w,x}.
 \end{align*}
 It follows that $2c - 2\ip{w,x} \geq 0$ if and only if $x$ is closer to $a$ than $b$.
 Therefore, we have that 
 \begin{equation*}
  \cbrac{x \in \R^n \c \nrm{x-a}_2 \leq \nrm{x-b}_2}=\cbrac{x \in \R^n \c \ip{a-b,x} \leq \tfrac{1}{2}(\nrm{a}_2-\nrm{b}_2)}.
 \end{equation*}

 
\end{solution}

 



\end{document}


